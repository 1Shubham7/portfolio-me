---
title: "Introduction to Kyverno"
description: ""
dateString: December 2023
draft: false
tags: ["Kubernetes", "Kyverno", "Kubernetes API"]
weight: 99
cover:
    image: "blog/kyverno.png"
---

## Introduction to Kyverno

Kyverno is an admission controller for a Kubernetes cluster. But it is more than a admission controller. Kyverno comes from the Greek word meaning "to govern".

The primary job of a admission controller is to validate the resources. If a resource is passed to an admission controller it will tell if it is allowed or not. And that is all what an admission controller does. But as we said Kyverno is more than just an admission controller. Kyverno acts as a admission controller and validates the resources but it also can change the resources transparently.

In this beginner friendly article, we will discuss what Kyverno is and how it works on theory, but before that let's have a look at some basic terms:

Custom resources
Custom resources are the extensions of the Kubernetes API that are not available in the default Kubernetes installation. These are like additional stuff or customization we can add on Kubernetes. However, many Kubernetes core functions are now build using custom resources, this make Kubernetes very modular. A Kyverno policy is a standard Kubernetes custom resource.

Custom Controllers
The custom resources lets us store and retrieve structured data. But when we combine a custom resource with a custom controller, it provides us a declarative API. Custom controllers are used to encode domain knowledge for specific applications into an extension of Kubernetes API.

Admission Controllers
In simplest terms, If a resource is passed to an admission controller it will tell if it is allowed or not. To explain further, the Kubernetes official documentation defines admission controller as - "An admission controller is a piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object, but after the request is authenticated and authorized." Admission controllers can either validate resources or mutate them, or do both.

Why Kyverno
Kyverno is not just another admission controller, it can do much more than that. Where an ordinary admission controller can only validate the resources in the cluster. Kyverno not just validates the resources but can also change the resources transparently. when Kyverno receives the API requests, it can validate them, this can also be done in blocking mode.

Blocking mode means it won't prevent or allow cluster to run but specify that in a policy report.

Other than this Kyverno also has the ability to mutate the resources transparently. Through Kyverno you can also generate Kubernetes resources. This means you can create new Kubernetes resources of any type or configuration based upon a policy that you define and install in the cluster.

Kyverno Policy
A Kyverno policy can be one of: 

Cluster scoped
Namespace scoped 
Inside the Kyverno policy we have rules. A single policy has one or multiple rules. Each rule has a match block and can optionally also have an exclude block. 



Kyverno

You can define scope of the resource you want to apply your policy to, using the following options. You can off-course choose one or multiple of these options:

Resource Kinds
Resource Names
Labels
Annotations
Namespaces
Namespace Labels
(Cluster)Roles
Users
Groups
ServiceAccounts
You can also declare what you want from the Policy, do you want to use it for Validation, mutating or something else. Here are the options you can choose in Kyverno:

Validate the resource
Mutate the resource
Generate a resource
Verify images 
Example of Kyverno Policy
Here is a sample Kyverno Policy, A "ClusterPolicy" applies to the entire cluster. you can see we have a single rules block inside which we have a match block. We have set validationFailureAction to Audit therefore, it will be allowed even after the resource validation failure what but we should get a report if the resource validation fails.

This policy will perform validation of the pods and check the label. If the label is not provided with any value then it will send a message saying "The label `app.kubernetes.io/name` is must" .



Screenshot-2023-09-29-072343

You can copy the same policy from the following code:

apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: require-labels
spec:
  validationFailureAction: Audit
  background: true
  rules:
  - name: check-for-labels
    match:
      any:
      - resources:
        kinds:
          -Pod
    validate:
      message: "The label `app.kubernetes.io/name` is must"
      pattern:
        metadata:
          labels:
            app.kubernetes.io/name: "?*"

Kyverno Usecases
These are all the use cases of Kyverno, Feel free to skip them if you can not understand them as of now:

Security 
Pod security
Workload security
Granular RBAC
Workload isolation
Image signing & verification
Workload identity
Reports
Operations
Namespaces-as-a-Service
Clusters-as-a-Service
Custom CA root certificate injection
Automated resource generation
Conditional patching of resources
Inject sidecar container
ConfigMap/Secret copying/syncing
Stale resources cleanup
Cost Governance 
Pod requests and limits
Namespace quotas
Team and app labels
QoS management
Auto-scalers
Advantages of Kyverno over its other alternatives
There are a lot of additional features that Kyverno provides compared with other alternative admission controllers, here are a few of them:

1. Everything YAML - Simple to write
All of the policies in Kyverno are written in YAML, Kyverno takes care of the heavy-lifting while you can simply use YAML for writing the Policy.

1. Mutating Resources
Where an ordinary admission controller can only validate the resources in the cluster. Kyverno not just validates the resources but can also change the resources transparently.

1. Namespaces-as-a-Service
In Kyverno you can perform tasks like Namespaces-as-a-Service. You can simply create a namespace with a bunch of other resources that are in it. Kyverno can then generate all of those resources for you based on the manifest. You don't have to run bash scripts or spit out resource quotas and limit ranges. You just have to simply  write a Kyverno policy in YAML, install it in the cluster and version it. Kyverno will perform all the tasks for you after that.

1. Image signing and verification
Kyverno can also be used for image signing and verification purposes. Kyverno can do image verification without needing other bolt-on components. The Kyverno policy will need to know which images you want to check. Whenever that image gets built, it will get signed and pushed to the OCI registry.

1. Some more features of Kyverno
Some more features of Kyverno includes:

Matching resources using label selectors and wildcards.
Validating and mutating resources using overlays.
Synchronizing configurations across namespaces.
Blocking any non-conformant resources using admission controls, or report policy violations.
Kyverno CLI - for testing policies and validating  resources in the CI/CD pipeline.
managing policies as code using familiar tools like git and kustomize.
Conclusion
As we discussed in previous sections, Kyverno is not just any admission controller, it is much more than that. An ordinary admission controller can only validate the resources in the cluster. Kyverno not just validates the resources but can also change the resources transparently. We discussed about Kyverno is this article we started off with an introduction to Kyverno, later we dicusses the terms like custom resources and custom controllers. Custom resources are the extensions of the Kubernetes API that are not available in the default Kubernetes installation. While custom controllers are used to encode domain knowledge for specific applications into an extension of Kubernetes API. We then say what Kyverno does? and an example of a Kyverno policy. At the end we discussed the advantages of Kyverno over other admission controllers and various usecases of Kyverno.

This marks the end of this article and we hope that this article helped you improve your understanding about Kyverno and policy management in Kubernetes.




`kubectl 


Kyverno is an admission controller but not just another admission controller. 
it as being part of the kubernetes control plane and every api request that flows through the kubernetes control plane goes through the api server goes through multiple phases starting with uh you know authentication then authorization and then admission review right so kubernetes is designed to be extensible and you can plug in your own you know admission controllers which is what kimrono primarily functions as there also is a cli which you know based on time if we can demo i'll showcase how you can use kiberno outside of kubernetes and kiran also does background scans right so if you think about it you have your build you have phase you have your deploy phase and then you have your run phase kiverno can operate at all three but it's um its main use case tends to be admission controls okay so what do you where do you want to start um do you uh i guess we start from the beginning how would you get converto into the cluster yeah so let's let's do that right so let's let's actually show what this looks like when we're running so if i go to the installation section of the docs you'll see there's you know some helm chart instructions for this demo i'm just going to copy and paste this one command line which pulls down all the yaml configs and what i'll do is i'm just going to use my local mini cube cluster um for the demo right so it's just right now doesn't have anything running known it's just got the basic name spaces and we'll go ahead and install kiverno itself so as you see once qiverno gets installed it pulls down a number of different configurations including there's a bunch of roles it configures so there's fine grain configuration there's a security section in the docs which explains what each one of these roles are doing it also installs some webhooks which is what tells the api server to in you know contact caverno based on certain settings and then it's creating some resources custom resources for policies policy reports so one thing unique about kiverno is both the policies as well as the policy reports are completely you know modeled in kubernetes fashion using declarative configurations again so you can describe them completely in the ammo makes it easy to manage through group cuddle customize etc right so quickly could i you know just like using explain for other things i could explain uh the kubernetes extensions now that absolutely no that's that's a great point right so if i do something like this if i want to see what exactly in the policy spec you know there's rules and there's preconditions what does that do it will kind of tell you and it gives you a link to get to more info as well right so um and if you have the right visual studio plugins enabled you'll get help directly in there as well because this is all driven through open api v3 schemas yeah so now that it's installed let's try and um you know um check the logs of out in the given namespace and just make sure everything's running right so what we see is kiberno came up it registered some webhooks again and you know i'll explain a little bit more about how these webhooks function but these are what tells the api server to forward certain requests to kiberno uh and then it also now it's saying you know everything's up and running right but at this point we have no policies installed because i went through the yaml and i just installed kiverno itself it's got a deployment it's got a bunch of other configurations and these are also you know documented here so it creates all of this within the kivarno namespace by default but we have no policies running right so if we go back to uh you know now at this point uh just to kind of showcase and first of all you know if we start looking at some basic use cases why do we need something like give or not right so if i well one of my favorite uh i guess scary demos in kubernetes is this one liner which i think i saw duffy cooley demonstrate uh in the kubecon or some conference where if you kind of look at what this is doing is it's running a simple image but then it's doing an ns enter and you know kind of uh also elevating privileges for the spot right so if i run this the interesting thing that should happen if it's allowed on my cluster which in this case i have no policies or no guardrails in place um all i need is permissions to actually um you know run a container in any namespace and now i have access and it put me into a bash shell based on what's allowed over here because we're running bin bash and if i do an ls it looks like i'm on at slash at a root file system right and if we go and kind of look at even you know var log um and let's let's try and see if we can get to warlock containers i see all of the containers running here and now i can see their standard out i can even go back you know if i go to proc i can see everything all the processes running so definitely not a good thing not something you want to allow on your clusters right so and this is just stock kubernetes if you install kubernetes by default without pod security policies or any other you know um uh it sort of controls or like a policy engine like kiverno this is what the behavior you would get so to to solve this you know we had pop psps or pod security policies uh in kubernetes those um you know as a lot of you may know got deprecated and a mark for removal in 1.25 and there is a you know built-in um alternative coming called pod security you know admission which will provide namespace level controls but doesn't allow the same fine-grained controls that psp's allowed right and do you know why or can you talk about why psp's got deprecated and why isn't something like this built in like it seems like there's a big gap there right so so going and i'll actually show what these some of these standards look like so one of the challenges with psps and and with any any implementation of pod security is um how do you configure how do you manage at scale what are the defaults you want to allow right so psps had a you know powerful way of configuring them through our back as well as um you know other mechanisms which are native to kubernetes but the challenge always was that you know you have controllers third-party controllers or things like deployments running pods so the role that might create a deployment is not that the same that's used as a role to execute a pod right so that was one challenge the other problem was just allowing exceptions and fine-grained configuration became a bit complex and and challenging and it never as as different you know security concerns emerge it was not a good way to quickly you know address those gaps right so the the interesting thing which came out of those discussions and there were kubernetes sigs like sega security um led this is they they're standardized on three levels of privileges within a cluster so there's privilege which allows everything so typically you do not want to run you know applications at privileged mode but then there's also baseline which is you know gives you some minimum restrictions and there's restricted so the the thought process was that now just to make it easy to have some secure by default option you can you can set up you know pod security admission and you can mark a namespace as baseline or restricted you can set baseline by default so at least you get some security but you know other more restricted policies which are in the restricted profile you would have to configure yourself right so in essence you know what what we typically would recommend is to say you want to start with restricted by default and then uh configure exceptions only when necessary but to do that and to kind of make that a default it just was deemed as very difficult because there are some valid use cases where you might want to allow you know some pods to have privileges and things like that so i think one of the things that's glaring here is that you know when we look at these policy engines they can look at the inbound api server request flow and they can look at that payload and see and they can make decisions about allowed or disallowed based on items within that payload that are not necessarily exposed using the the new psa pss standards and tooling right yeah and the other challenge with using psa is you know of course you want to get uh reporting else in a native manner as well as you want to also um as your kind of scaling usage you may want to kind of allow you know this the configuration of exceptions uh uh other things again in a very like like you were mentioning too jimmy in a fine grain matter um as needed right so and not not just at the namespace level so if you're running larger namespaces which include multiple controllers it becomes difficult to you know you kind of have to um subname space or use some other mechanisms and you kind of now have more name spaces to manage so at this point we've got caverno installed can we see it in action see what it does yeah so let's go ahead and apply you know pod security policies right so and i'm gonna apply just the default from here and what you'll see is now if i do um you know get i see a bunch of policies i see they're in audit mode right so what just happened is because they're in audit mode and in a few seconds these will all go ready so by the way when they are ready it means now kiverno has updated the web up to start receiving events for this policy when these are in audit mode um let's say if kirono happens to be down and not reachable the api server will still continue but if i flip him to enforce mode uh what's going to happen is it's going to you know those will actually request will be blocked right so actually let's let's do that so i'm just going to change this customization um you know over here and we'll reapply these policies in enforce mode right so so it's very similar to like sc linux right i could have a ability just to like hey i just need to know what's what would be a problem and and what you know and then later on i want to switch to actually enforcing and say like hey actually no not allowed anymore well yeah you bring up a good point i think when you're introducing new policies in general to your clusters it's a good idea to be able to understand the impact right so it's a great idea to impact analysis but there's other things there's other reasons why you have these audit policies as well maybe we can talk about those yeah so there might be you know i mean so certainly something like pod security you do want to enforce but uh like best practices which maybe just you know good good thing good hygiene for your cluster but maybe you don't want to enforce those or you could choose maybe in your dev test the product cluster you you know you audit and then in your production clusters you enforce so a lot of flexibility in terms of how you can manage this but just you know now if we show the same command again right we already have that part running by the way but uh the same command you know was blocked by qivarano and shows a number of different violations right so everything from uh it's saying you know if we did a drop you need it wants you to drop capabilities within the pod uh host namespaces are not allowed so you can't you know register a pod in there privilege escalation um you know disallowing again any container with privileged mode uh as root user that's blocked as well as requiring some sec comp configurations right so all of this is just you know again as you see like in a few minutes you can go from like zero to fairly secure for your workloads and in kivernal policies one thing i should have shown in the docs is there's a lot of flexibility in how policies and rules are organized so you can match and exclude based on kinds names labels namespaces roles usernames even you can look up things from the api server and you know have preconditions to filter rules and then you can apply either either verification or mutation validation of different resources or you can generate resources on the fly and we'll look at those use cases but the the goal here is because now you can have these you know exclusions and exceptions again make it very simple to have fine grain uh control over how you're enforcing pod security right so just again simple way of uh looking at this and and at admission controls again you you will see this uh and your pod insecure pods will get blocked right away what i'll do next is i'm going to actually flip these back to audit mode and the reason why i want to do that is i want to show what the reporting looks right right so let's go back to him with the customize command and i'm just going to rerun it with you know part security so that should flip it back into audit mode verify by looking at this yep so all the policies are now in audit and what i want to show is um if i look at so cpal by the way is a short form for cluster policy so um n p o or palahar is a short form for policy report right so like i said both of these are you know native resources and if i do so there's nothing in in their namespace too which is nice because now developers can get the reports um in their own you know namespace uh if needed right so if i run let's say now that everything's in audit mode i can just do something like let's just run an nginx pod and i'm not going to try it on it i'll actually run it and if the pod got created now if i look at policy reports immediately it's telling me that 15 rules passed but four failed so if i want to look at this further and see what exactly failed let's look at it in yaml and i can see exactly which rules failed now it's telling me again these are all the pod security policies which we had just configured um and i can see which those fail one one other thing to quickly show here is if i just even if i want to create a deployment um so let's try that we'll say create deployment and i'm just going to drive on this so it doesn't create resources and in this case oh because we have them in audit mode yeah maybe dry run is not the best idea we'll put them i'll go ahead and create the resource but what i should see now is there's more violations even for this new resource even though i created a deployment right and this highlights one of the features in kiverno where you can write policies at the pod level and by default this will it kiberno will automatically generate rules for standard controllers right and you can customize this as well i think that's exactly what waleed was asking for here as far as like reporting uh and comments about like how do i if my pods are being you know hit with these policies or or you know not not complying to good policies like how do i know what created that pod is is actually difficult sometimes you have to find the deployment or a job or cron job or something like that and bubbling that back up to like the top level thing uh is great because then you can actually track down like okay who owns this how did it get deployed here that's a lot easier than starting at the pod right yeah and if you describe the pod let's check that one of the pods and if i describe nginx i'll also see that you know in the events itself caverno kind of inserted uh sort of any policy violations so again as a developer if you're looking at your own workloads you've deployed that you can look at the policy report and you can also see this within your pod itself and similarly if you mutate things if you change things that will also show up both as an annotation and it will be an event to show that within the resource itself but there's a there's a lot to be said though for being able to generate those workload policies by just writing physically a pod policy um because a lot of times we are deploying workloads right using you know deployments and jobs and daemon sets and we're not strictly going in there and just building out a pod and the user experience is better if i can immediately know at the client call that i've got an issue right yeah it makes it a lot simpler and and you don't have to then manage this for you know all the different controllers um and you can also by the way it supports custom controllers as long as they are following the same pattern that the internal controllers use uh you can you can have like uh argo deployment or other you know other kind of controllers as well yeah so those are just some very basics you know for getting port security up and running you know and monitoring uh and managing there by the way there is also a graphical kind of tool and i can i'll quickly show that because it's it's worth calling out so if i go into reporting i want to kind of quickly show you that you can also view your policy reports using this tool created in the community called policy reporter so internally the the standard the crd that um you know kiverno uses is um something that's you know being uh discussed in the policy working group there and it's being standardized by the policy working group right so the the nice thing about that is now you can get policy reports from kiverno or you can get policy reports from you know you know other tools which support the same format and you can show them in you know any tool either in cluster like like what we'll see over here or if it's an external tool um you know like a management solution you can also then see policy reports from kiberno as well as there's adapters already for falco um coupe armor there's also adapters for uh coupe bench uh so all of this can be seen in a very standard manner right so that brings up a good question for me like when at what layer do i want these different tools right i mean like if i want to use falco or i need falco like oh do i also need gaverno do i need something else like how it's interesting that they all work together and great but right but curious like what the layers are there of just like when do you need a different thing besides just policy yeah good good question right so policy is of course a fairly general term um but here um you know so kiverno does configuration policies but cavernot doesn't look at you know runtime occurrences like for example if somebody um well if someone execs into a container that also goes to the api server but let's take another example if somebody you know kind of is inside a container and they're allowed but then they try to mount a file system things like that which you can pick up through ebpf right so falco and celium and cube armor they all kind of look at ebpf for runtime events uh and they have the ability to kind of manage things uh policies at that level so caverno is more admission controls and configuration policies whereas that class of tool is more runtime so um would we kind of advocate uh you know at nermata is you you're going to need both right but obviously you want to kind of enforce this as early as possible you can't wait to run time because it's already too late i mean you're detecting things rather than preventing things right so ideally you're preventing things in the configuration and there's a lot of best practices and things that can be applied there but then runtime is another layer of security you can additionally add so to make sure you understand it then i've got maybe you know secomp in there or app armor to prevent um unwanted let's say sys calls from from user space right down the kernel space and then um i've got caverno on top of that to prevent anything from entering into the cluster that would make those unwanted calls right and now i may also have falco in place that's going to allow me forensics collection and and look at what's actually happening in my runtime environment so what i've got now is a defense in depth approach right right verno is blocking you know the the inbound um and then but if anything gets past that for whatever reason i've got other layers that i can rely upon for for runtime forensics and security absolutely yep yeah so this tool policy reporters picking up right now we saw the in a kind of violations we can see them in the different name spaces pretty slick tool you know created by frank uh joshua who's one of the caverno maintainers um and he came he manages and maintains the this you know which he has donated into the caverno project uh but yeah beyond kiverno it can also pick up policy reports from these other tools um so yeah definitely check that out if you're looking for a you know simple way of like looking at this per name space or across an entire cluster caverno does all that i'm sorry so what about downstream like you know i've got a sim that i want to you know put things into our like i know that i've been to a couple of the working group uh meetings and right it's uh uh there's talk about oscar integration and i mean can you speak about that real quickly yeah yeah so now that there's a you know emerging sort of standard for these policy reports and violations i mean all sorts of things can be built on top of that so there's of course you know management solutions like um openshift the red hat advanced cluster management as well as nermata my company provides like management plane uh tools for multicluster uh there's also as well as you know governance and management of policies uh and there's you know kind of other tools where you can you know the integrations can be to push things into other cm tools or even notification tools etc right so and that can be done either from each cluster or centrally through a management uh policy administration point okay so what are you going to what are you going to show us yes i'm one of the things i'd want to see is uh and i talked to you before this generate policies right right yeah so let's take a quick look at that right because that's a very interesting and unique use case so what i'm going to do is i'm just going to let's clean up some of these um policies and we'll kind of switch to the you know generate use case itself right so given all as it you know works as an admission controller what it can do is in addition to you know validating enforcing you can have policies that look at different thing resources coming in like for example if a namespace is created kirono can look at that and you know either mutate and you're it's like creating a standard label to say who created this namespace right or you can also then you know generate things like for example you want to generate a default network policy right so you want to deny everything by default and then allow your dev team to maybe add specific you know network policy rules or you could do things like if you have the right label on the namespace using that to trigger creation of a central network policy like an istio policy right so lots of interesting use cases around that but what i'm going to showcase is just how can you do like multi-tenancy continues to be a big problem right and it's a topic i'm you know personally very interested in and i kind of contribute also in the multi-tenancy working group in the community uh salt showcase how can you do some lightweight multi-tenancy uh with kiverno right so what i'll do is i'll and just before we start like the the alternative here or the way that a lot of people do it is namespace creation is controlled by a team right like a lot of times like hey if we're gonna share this cluster that team over there is the one that they have to set up everything because they have to make sure the tags are right they're gonna do all that stuff manually yeah and so in this case you're saying now let's just create a policy that automatically mutates anytime you want to create a namespace and then we can make sure that those policies and everything else are there correctly we're not gonna be able to create a namespace and and you're freeing up the team to be able to do other things and and making sure that that is everywhere otherwise it doesn't get created exactly yeah so you're in these policies um you know can do you can do some pretty interesting things right so just as a demo uh what i'm doing here is i'm saying a paul every namespace has to have a certain set of labels and one of the labels is the type which i've just created like small medium large right so let's say three t-shirt sizes and it's up to me what i define those as being so of course based on application you can customize this however you want these are policies you're managing but once i set this up now i'm allowing you know self-service and i have a couple of roles i created here which i will include and so there's ned and nancy these are namespace admins that i want to allow them permission to create namespaces right but when they create a namespaces i'm still applying policies and certain rules to restrict what they can do and to kind of guide and govern the behavior there right so that like you said it eliminates all that manual handoff and that you know it allows also allows some standardization and automation so what we've seen is you know teams uh just completely automate all of this through you know git requests so you can create a pr in a repo which would trigger some automation to then create your namespace so all of this is audited recorded but you now have your platform team setting up the policies developers coming in and saying hey i create a pr to get my namespace i you know once i'm done with it i can delete that namespace and it's just nice and simple and easy to manage yeah so let's take a look at an action right i'm gonna just apply this um and immediately it shows me first of all this is a good thing um you know it's uh i should uh have pointed that out earlier but kevorno by default does not have permissions to create things like cluster roles etc right but we do want to give it permissions to create fine-grained cluster roles so it's telling me that hey there's this you know remember when we install cabernet there are a bunch of you know roles and things which got created and i said if you look at the security docs it'll tell you what each one of those are and how to manage them so here what i want to do is i want to edit this cluster role generate right and if i look at it by default it's allowing some namespace level configurations but it's not allowing you know in the our back thing uh it's not allowing cluster you know roles and role bindings right which is what we in this case we want to allow that so let's try and give it those permissions i'm going to say cluster roles and cluster role bindings and we'll save that and try this again just to make sure we can go through now without any errors right so good so everything got created and if i now look at you know um so by the way that validation is custom validation and given all right so it's just helpful that it tells you that when the policy is deployed rather than troubleshooting you know later to say why didn't something work right so at this point if i create a namespace right so and let's try and create this um as as a specific user so um we'll say create namespace test and we'll say as you know um let's say nancy right so if i do this oops forgot my create so we'll put that it in and say once i create this namespace it's telling me right away that hey you know nancy is allowed to create the namespace but we have this policy which says you need to give up tell me what type right and i'm not going to default the type and you could default the type if you want but in this case i have not so it's forcing you know giving a type so uh to kind of you know uh manage that what i have is if i look at the resources and there's um you know there's a namespace so over here so let's let's just quickly look at it so this has a you know label which is a simple name space which just says the type is medium so let's go ahead and you know apply that so if i say group cuddle oops and we'll do apply and we want yeah this resource and we want to do this as that user right because of course as an admin the policies are configured so an admin can do create namespaces but users cannot so if i create this as that username see now this time it went through because i i met my requirements and if i look at you know this namespace so let's let's say as nancy um i should be able to see the namespace but if i look at it as ned it says i'm not allowed to see that namespace right so what kiverner did behind the scenes and what i have in my policy is there were a couple of roles fine-grained roles that kivarno created right and i'll just quickly show what those look like because they're interesting so it created a cluster role called called in a namespace owner and then it also created the namespace uh let's actually let me find the yeah this is what i was looking at so so it generated the namespace owner rule and it's saying that only you know it's a nancy now has permissions to only view and delete that particular namespace um and other users like ned did not but you know and it also create gave me permissions you know to to be able to go as a namespace admin to do things inside that namespace right so all of this got generated and we also had a policy you know to do this network policy right so let's let's quickly check on that so if i say namespace let's say get will do minus and test and we'll describe quota or let's first start with net network policy whoops so yeah it looks like it you're not allowed so that permission that rule would need to be added but just to show and this can be you know it depends on how you want that right so if you don't want that even the namespace owner to see the network policy you can you can manage that if you want the namespace owner to see that because it's a generated resource you can allow them to do that but if i view it as an admin i can see the network policy is created and everything is blocked by default right so now that user can create more network policies for their app to allow certain routes but by default it's secure and segmented uh based on this configuration right so again simple way and a few setup policies all of these by the way everything i'm demoing is in our sample library too so if you go back to the kiverno page and go up let's go to the give or no io and if you click on policies there's about and these are all thanks to the community those where almost close to 140 different samples you can kind of browse through them based on the type of policy as well as you know which resources they apply to and more and more we're coming up with policies specific to different other open source communities and tools uh as well as you know certain use cases so if you have requests if you have like ideas for policies feel free to you know reach out on slack or you know on the github uh itself and showcase those so if i had a we're talking about multi-tenancy here and locking down name spaces by default but if i had a desire let's say to manage my multi-tenancy in such a way that i'm on the same cluster but i'm running my pods on my own set of compute let's say tethered to a namespace yeah i could use generate policies to go out there and and create the namespace and then and actually go out there and create um you know a set of let's say tolerations and node affinities that get applied to inbound uh workloads as they hit my namespace right i mean i can create those right yep yes so you can you can kind of target certain nodes based on either labels in the workload labels in your namespace um you can you know even yeah so you can inject node selectors so there is a you know so either through taints and tolerations or node selectors i think there's an example of that so those are you know all valid use cases yeah so this will inject a node selector automatically based on other things in that configuration right so you could say if if it has a certain label go ahead and inject a name node selector or if it's from a certain user you could do it based on that as well and to be clear it wouldn't be good enough to mutate the inbound requests i'd also want to have a validation rule a policy that would say hey based on this namespace do i have the right tolerations and node affinities in place so that it you know doesn't go into someone else's namespace right there are a few questions i wanted to bring up that were a little bit old now but i kind of i wanted to still ask them sure is there weights and nest policies or have some sort of like priority between policies mohammed asking yeah so each policy itself so policies contain rules rules are you know executed in order right within a policy but there's no ordering between you know the policies itself and that's by design because we want each policy to you know be atomic and and kind of fail or succeed in isolation from other policies so you won't you would not be able to override a policy somebody else has configured but you would be able to you know kind of again you can add exceptions into that policy but you'd have to change that policy's definition cool and and walid was asking about jimmy we brought up earlier like the layers of who we want foul code we want caverno and policies uh doing i'm not exactly sure what wally's asking for like which portions he wants to do earlier in the stack but in registries that's where you get like security scanning just to see like obvious one abilities known cpes that sort of stuff is where you're gonna actually do some scanning in a registry itself and not necessarily in the kubernetes cluster at all because that's we can't detect necessarily what the image i mean i guess we could rip apart some of those image specs and see what they're gonna do um if if we know that like there's a host mount volume or something um that like it tries to do but it's all of that is runtime configuration not build configuration so for me for me and and i i'm open to people disagreeing with me but for me it's a matter when i say defense in depth i also think about do what's what you're supposed to do in each one of those those tiers let's say so you know um caverno has a cli as well right so i could do some cli work in the cicd portion of of my let's say my quote-unquote supply chain and then you know when i get to the images a registry is going to scan it's going to encrypt it's going to control how it's vended you know and so that's where i'd scan my images and then when i get my containers actually running i would do run time container security which is where caverno sits at the front door preventing unwanted behaviors and but other tooling would also detect you know drift or you know unwanted binaries there's a lot of tools out there from that are open source as well as from partners that can you know so it's it's not a matter of either or it's a matter of all them together and some of them may duplicate some of of up different layers but i think that's the whole idea behind defense and death yeah now great great explanation there and and certainly i think having the command line tools to run in ci cd pipelines being able to do this as early as possible is necessary so one thing i wanted to showcase though and this is a great segue into supply chain security so we can talk about what kirano can do with container image signing with even attaching things like s-bombs or vulnerability scan reports lots of exciting things in that space but before we go there one one interesting quick demo i'll i'll do right so this policy is actually looking up the image config data so it's it's going to the uh and so kevin can do external calls but it's not designed to be an open system it's for kubernetes so the way this works is there's built-in integrations to certain things like oci registries right but those integrations the idea is to make it super simple then to check for things like this policy is saying if an image is more than six months old don't allow it right so i want to make sure that my images are kept current and for certain you know workloads i can enforce you know whatever period makes sense right so a year six months a month etc and that's going to apply whenever i try to create an image or create a pod in the cluster right so if i had a deployment that sat there for six months that i never updated the image yeah but i knew that all of a sudden if i if it nothing's gonna stop it from continuing to run but if it tried to scale out it would fail right new pods would not be created because those are against policy but the existing pods are fine so for the existing pods it will show a policy violation right so you have that alert first off and with 1.7 a features coming to mutate existing resources which will actually let you scale down that workload if you want right so that completes exactly the scenario you were thinking of where um but in most cases you're not going to want to you know tear down your production workloads you're going to want to alert and say the importance of audit mode yeah so so that's where you know the idea is that at least you can audit on it very easily uh and once you audit on it it's up to that you know team to kind of take the right action yeah so the previous example you showed though where was looking for time since created that's available from a generic oci registry it's not like any oci registry um yes um anything which you know can uh so basically and i'll show you you know what that config looks like so if i once i apply that policy um so let's say if i want to do group cuddle and run nginx if i do image latest of course it'll be you know it'll be fine but if i want to run let's say if i remember the versions correctly i think one dot let's try 1.13 i know that's pretty old because engine x is probably at 1.21 we'll just do a dry run here because this policy let me double check but i'm pretty sure it's in enforce mode yeah it's in force so that way it doesn't create so we'll say dry run equals server right so we're just saying run nginx do that and what it's going to do is it's actually going to fetch that and it says hey this one's pretty old don't use that but if i do if i recall correctly 1.21 is more recent which let's see if that works well i already have an nginx but it went through the web book and it allowed that right so that's and to do this if you kind of see what exactly is it looking at so if i go back over here and if i just say you know in crane i can kind of look at crane as a command line tool to kind of inspect oci registries so if i just look at nginx latest there's a bunch of stuff that you can pull down and it shows me exactly what this how this image was so let's actually make that a little bit prettier um we'll pipe it to jq and yes there we go so we see the layers we see everything that was built so you can even do things like you know there's example policies to restrict volumes right so that's something you just mentioned um you know justin if if you want to kind of now um let's see i don't have that here but it's uh in the um let's find it here so you can kind of restrict things you can even do things like resolve you know image tags to digest you can block you know large images this is the one for which blocks volumes so all of this is not possible because this is how you built the image right and you're writing policies on those so again as a platform team you have a you know crazy amount of control in terms of what you want to allow and what you want to deny and how you want containers to be built like one of the questions we we had in the community which is an interesting one is hey how do i prevent somebody from overriding a layer right so now you can check and see which layers were added and if it's something which you want you know in in your base image to enforce and not to allow that to be over it and you could write a policy to block that yeah so that's that's just on you know the image config but very quickly i want to show also some policies which will allow you to do more than just uh check for config right so this policy is checking to make sure your image is signed and caverno has a built-in integration with uh six stores cosign so you know six store is another linux foundation community um and their focus is on supply chain security so from six store there's a bunch of open source tools cosign for image signing and verification uh there's a tool called recore which is a transparency log to keep metadata and things and then there's full co which is a web-based you know identity management solution to offer what's known as keyless signing so you're you know then this by the way says check image keyless so just to explain what i'll do in this demo is if i go and look at the repo that i want to use it's just a java tomcat repo and if i look at the actions here i've built a github pipeline which as part of that pipeline it creates a scan report it creates an s-bom using cyclone dx format so this is once the image is built and then it's signing all of these using cosine and it's putting image signatures if i go and look at my in a registry year i'll see that it has you know image signatures um also attached to it right so i want to go look at all the versions and i'll see that at least version 0.0013 has image signatures but v1 does not right so let's apply this policy and see what happens you know once we you know require that for our cluster so i'll clear this screen and if you look at that what i want to do is we'll do check signatures and now if i run tomcat and let's try v1 first which should get blocked if everything works correctly which it did so it immediately said that hey there's a signature you know failure because there's no signature on that image right but if we kind of go back to i think it was zero zero one three um if we do that oh it so that should now get allowed because that image is signed right so very kind of simple quick demo but shows the power of you know signing images is another um looking at defense and depth and security best practices this is quickly becoming something that you know is highly recommended and it's very simple to do with cosine and six store as well as enforced with kiberno so it is it is it's doing two things here right it's verifying that a signature exists and it's also checking in against the certificate or yes so it's checking and it's ensuring that it matches so in this case just to explain a little bit more about how i had my repo set up so what i used in fact in in my action year if i go into one of this is what's known as that keyless signing right so what that means is i'm creating my scans my s-bombs and you can create other metadata as well but here when we're signing the image i i did not give it an identity but what i'm telling it to do is to say use the feature of generating an oidc identity to be able to sign that image and that's where it you know so when i'm verifying it i'm using my root certificate for my um for full co to say anything signed by that is allowed but i'm also going to check for additional metadata right so looking at a more complex example of the policy you can do things like you can check and see hey i want to make sure that this this image has an s bomb and it has to be in cyclone format or it has a scan in this case from trivi right or i can check and make sure that you know i have a certain score you know no vulnerabilities allow uh higher than eight um in in if i'm using something like vex format or in this case again i'm using you know trivi but you can customize this as needed right you can also check for other provenance data like who built this which branch uh things like that that's really neat to see the integration there with a registry scanner because historically it was always like okay we set up scanning on the images and then we just we just trust someone else that verified the reports right or said yeah or reported on it like saying like hey by the way like you have a critical you know a 10 cve like maybe don't deploy that but but there's no actual enforcement there it was always just like kind of emails and and back and forth between people saying like hey i checked this report here's what it said you need to go fix it and now you can enforce that directly at the at the don't create this thing unless it's past these checks right and by the way if you know what we recommend is that your scan reports are updated like daily or weekly or some frequency that makes sense to your organization and you can do all of this now so previously there were some attempts at doing this in cluster which don't doesn't scale very well right and then you have to rely on every cluster to be working and this is more points of failure so ideally you would still run this as a you know cron job outside of the cluster but once that those scan reports are refreshed you can then verify inside the cluster even flag workloads which are out of compliance or you can do things like hey i want to find any package you know using my s-bom uh any any you know use of things like log4j right just to go piggyback on a popular example right now uh those are it's all possible once you have this metadata attached in your image registry that's really cool yeah yeah so you know several several use cases around that and i know we won't have time to go much deeper into that with attestations etc but very quickly one more thing i want to show is how you can you know also do things like jimmy mentioned uh outside of the cluster using the uh the kiberno cube cuddle plugin and the cli right so a couple of use cases we see there first of all just applying policies right so if i want to apply so here i have a simple policy which is saying i don't want to allow the use of latest tag right so if i say you know creep cuddle and let's say group cattle caverno and we'll go with apply um this thing so first of by the way i can apply it on a cluster so if i do minus minus cluster it's just gonna use my group config and it's showing me that policy applied on every matching resource in my cluster right so extremely powerful way of testing a policy or you can just apply it on a resource if i don't want to apply it on my cluster and i say you know this is the party ammo year so it's showing me that you know by the way it's four because of the auto generated rules uh but you know it has one pass one fail um which is what uh you know i would expect you know this pod there's an error in here because it's uh it is using latest um which is why it's getting flagged right i can also uh a similar command here is if you do cuddle if you're writing policies and you want to write test cases for it let me switch to the right directory here so i'll go and show an example of how we test things within the kimono projects itself so let's run the plugin and if you do test it will show me all of my policies applied to different resources so you could have success cases failure cases and it shows me how many tests were applied and the unit tests kind of for this right because it's important as you're authoring policies applying proper policy as code making sure there's unit tests uh and again all of the samples have unit tests so you can start with that but you can quickly get to you know writing your own custom policies and managing those and that's applying it against the local resources or a cluster oh it you could do either right so in this case these tests are applied in in my case against a local resource and i'll just quickly show what these tests look like so but you can it's up to you how you want to apply those so there's these are my test cases and i'm just saying which deployment which policy and what my expected result is it's very simple to manage and you can see there's a bunch of different test cases on the same policy of success failure and different combinations cool all right well that was that was awesome i i learned a ton on on the different ways you can use caverno where it interacts with other tools external and and part of your csd pipeline so that's that was awesome thanks so much jim you're welcome my pleasure yeah thank you very much jim if anyone else has questions and wants to follow up where's the best place to kind of reach out and get help with caverno or getting started yes if you go to kiberno dayairo click on community it has our slack channels those are the easiest and most of the maintainers hang out there um so we have a lot of end users in the community too so just you know feel free to chime in on the slack channels or you can you know also on github uh at just you know kivarno github slash cabrano go ahead and you can file issues or just reach out over there as well awesome well thanks so much for coming on the show um jimmy thanks also for joining us has been great yeah and everyone else thanks for watching we'll uh we'll catch you next time alright take care bye [Music] the