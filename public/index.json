[{"content":"About the internship The Linux Foundation Mentorship Program (LFX Mentorship) is 3 months paid internship/mentorship program where people contribute to open source projects. The program aims to foster innovation and talent in the open source community, and to help developers get job and internship opportunities after graduating. The program offers part-time and full-time mentor positions, and mentorships for projects such as CNCF, GraphQL, Hyperledger, LF Networking, Linux Kernel, OpenHPC, Open Mainframe Project, and RISC-V\nI worked as an LFX mentee for the project KubeEdge under my mentors Yue Boa and Fisher Xu. In my mentorship duration I worked on improving the test coverage of the project by writing unit tests, E2E tests and integration tests. I also worked on improving the KubeEdge documentation as well as migrating some tests from standard libraries to more convenient and faster assertion librabies like Testify. Learnings:\n","permalink":"http://localhost:1313/experience/lfx/","summary":"\u003ch2 id=\"about-the-internship\"\u003eAbout the internship\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eThe Linux Foundation Mentorship Program (LFX Mentorship)\u003c/strong\u003e is 3 months paid internship/mentorship program where people contribute to open source projects. The program aims to foster innovation and talent in the open source community, and to help developers get job and internship opportunities after graduating. The program offers part-time and full-time mentor positions, and mentorships for projects such as CNCF, GraphQL, Hyperledger, LF Networking, Linux Kernel, OpenHPC, Open Mainframe Project, and RISC-V\u003c/p\u003e","title":"LFX Mentee - CNCF KubeEdge"},{"content":"Contributing to open source, specially to the Cloud Native Community Foundation (CNCF) has been one of the most effective ways for me to learn programming, understand how large codebases work and work with experienced engineers on large scale projects. It\u0026rsquo;s been a year for me as a contributor in the CNCF ecosystem and I also have recently completed my LFX (Linux Foundation Mentorship) program as a mentee. As I wrap up my LFX program I want to share my journey in open source - from contributing to CNCF projects to getting into the prestigious LFX (Linux Foundation Mentorship) program and completing it successfully.\nMy first contribution to CNCF Let\u0026rsquo;s start it from how I got introduced to CNCF, I got know about CNCF through YouTube videos, I saw some YouTubers in cloud native space talk about CNCF and I went to the CNCF home page but I couldn\u0026rsquo;t understand anything, I think the reason was that I had never anything similar to CNCF. I was confused whether CNCF is a company where I can get placed? or is it a project? or is it like a community? But as I leaned more about cloud native technologies like Docker, Kubernetes I understood what CNCF is and what it stands for, by this time I also started searching of some CNCF projects to contribute to and one such project was CNCF ORAS (OCI Registries As Service), basically it is a tool for pushing and downloading OCI Artifacts from the OCI Registries. I contributed to ORAS for a month where I learned the how to contribute to a CNCF project. And with my ORAS experience I can suggest a newbie in open source that the best way to make your first contribution is to first, understand what the project is by reading docs and try using it, then using docs set it up locally and then look up for issues labelled \u0026ldquo;good first\u0026rdquo; or \u0026ldquo;help wanted\u0026rdquo; on their GitHub repo and start by solving those issues.\nThe first contribution I made to ORAS was some basic changes in the readme file, and then I tried understanding the project, watching tutorials on the CNCF YouTube channel, going through docs and trying to set it up locally. I then started contributing to the docs and then made my first code contribution in ORAS.\nApplying for LFX for the first time I contributed to ORAS for a month and I wanted to apply for LFX with them (this is during LFX 2023 term 3) but they did not have a proposal for that term so I had to pick up a different project. Later I realized I need to improve my skills before applying to LFX so I skipped that term and rather I went forward with an internship offer I had from a startup.\nContributing to CNCF Kyverno For next few months along with my internship I was focusing was on building projects, learning go and Kubernetes, and leaning about another CNCF project called CNCF Kyverno, it is a Kubernetes native policy engine. If we go back a little bit, when ORAS did not come for LFX, I started searching for other CNCF projects I that is when I saw KubeEdge, the project I am currently working under and Kyverno where I became an official contributor later. I started learning about Kyverno, one of my friends recommended me to contribute to Kyverno and I would also recommend any newbie who wants to start contributing to open source that you should choose an active project (where maintainers are active and regularly help contributors). I can recommend Kyverno to any beginner because it has a really good documentation and maintainers a very supportive. Luckily all three projects I have contributed to till now have been amazing. Also I would suggest that at KubeEdge, we are continuously trying to improve our documentation so if you think you would want to help us, please join the community and contribute.\nTip - If you want to start contributing to Kyverno my recommendation would be to start by watching some CNCF talks about what Kyverno is and what it does, then complete the free certification from the Nirmata website and then pick up any good first issue from the project (they are very active and frequently post up good first issues to solve).\nEven I did the same, I also went through some CNCF talks about Kyverno, completed the certification and then started contributing. I started with some code contributions and then also created a section in their documentation about admission controllers.\nApplying for LFX again I contributed to Kyverno for a few months while also managing my internship and college classes and finally the time came when projects for LFX term 2 2024 were announced. I saw the list and Kyverno was not there and once again I had to change my project just before a month from LFX. I was going through all the projects that were coming in LFX, I saw KubeEdge there, KubeEdge is a Kubernetes native edge computing framework. I had some idea about what KubeEdge is and I found the projects really interesting so I decided to apply for LFX under KubeEdge. I applied for two projects, one was about writing new documentation and the other one was about test enhancement. While I was more interested in code contributions but I had a background of working as a technical writer for 8 months and I also had good contributions in Kyverno documentation so I decided to focus more on the documentation enhancement project and also give time to learn things for the tests enhancement project.\nFor the documentation improvement project, I went through all the tutorials present online for KubeEdge, I went through the entire documentation and I tried improving the documentation by bringing in changes in sentence refactoring, rephrasing docs and some other improvements. majorly, I wrote some blogs for different KubeEdge releases from v1.10 to v1.17, for this I used the changelogs of these releases as reference. While for the tests enhancement project, I started learning about writing unit tests, E2E tests using ginkgo and gomega, TDD (Test driven development) and tinkering with some libraries I could use for testing in golang. I had about a month to contribute to KubeEdge, understand the project and prepare two cover letter for the two projects. By the time LFX registrations started, I had good number of Pull Requests merged in KubeEdge (11) and some were being reviewed.\nAfter this, the tasks I had to complete in order to complete my LFX application were - submit a resume, and submit a cover letter (which is the case for most of the projects, some projects can also ask for another task testing your knowledge about the tool). I already had a good resume prepared so I made minor changes in it, I started preparing my cover letter for both the projects, in the cover letter I mentioned my past contributions to the CNCF community as well as to Kyverno, other than that I answered honestly all the standard questions that are asked, other that this I also mentioned how I would go about working on the project for the three months.\nAnd I finally got in! With my past experience as a technical writer, and the contributions I made to the KubeEdge documentation as well as Kyverno documentation I thought I had more chances of getting in to the documentation project, on the result date I was surprised for good that I have been selected to the \u0026ldquo;tests enhancement\u0026rdquo; project, this was even better because now I would get to make code contributions. I was super excited to work under my mentors. I just got started by revising some stuff related to unit testing and E2E testing, and then jumped into writing tests.\nMy LFX mentorship Experience The first half of LFX mentorship After I received the acceptance mail, I was really excited to get started with my mentorship, I communicated with my mentors and they told me to start by writing unit tests for simple files since those are more straight forward. This was the first time I was writing unit tests so I started by watching tutorials and deciding which library to use for testing. In KubeEdge we were using the standard testing library for unit tests and if else statements for assertion. I researched a bit and saw that using an assertion library like testify/assert would make more sense as it removes much boilerplate from the tests, and improves speed and quality of understanding and modifying the tests. And that there are negligible downsides to using this library. I talked with my mentor regarding this and we decided to go forward with testify/assert.\nFor these 3 months I was writing unit tests continuously but also working on other tests side by side, in the first week other than writing tests and researching I also wrote a release blog for KubeEdge v1.10. For the next few weeks I was only writing unit tests. I started with simpler and more straight-forward tests and then moved to tests that require mocking and tests that would test the integration (calling) of multiple functions.\nI had never written E2E tests before this so I also had to watch tutorials and read documentation for E2E. Our project was using Ginkgo and Gomega for E2E so I learned about them, went through the official documentation for Ginkgo and wrote some E2E tests for dummy projects for practicing. Before the first evaluation (which happens after 1.5 months) I had improved the test coverage of the project by about 5%, written E2E tests for adding the liveness probe configurations in deployment and checking if it works in the edge, and written a few blogs for some KubeEdge releases (v1.10 ~ v1.17). And when I got my mid term evaluations, I had successfully passed them.\nThe second half of LFX mentorship In the second half of my LFX mentorship, I continued writing unit tests, the only difference being they were getting more complex now and I regularly had to visit Kubernetes docs or https://pkg.go.dev/ for reference. During this time, I wrote E2E tests for verifying that the applications configured with probe works fine on the edge. Other than that since many tests in the project were simply using standard testing library without any assertion library, I migrated those tests to standard library + testify/assert.\nBy the time my LFX mentorship period ended, I had improved test coverage of out project by 10%, written about 20k lines of tests, migrated tests from standard testing library to a new assertion library and written blogs of 7 KubeEdge releases (v1.10 ~ v1.17). I also had my end semester exams during the second half of the mentorship so for a few days became very hectic but overall the mentorship went quite smooth and I got to learned a lot of things. Whenever I needed any help I communicated with my mentors and they were very supportive and helped me wherever I needed them. And with this on 29th August 2024 I received my final evaluations and I had successfully completed my LFX mentorship.\nFinal thoughts The 3 months of LFX mentorship was one of the best learning experience I ever had in tech. In these 3 months, I explored how to use KubeEdge, worked with my mentors to improve test coverage of the project as well as contribute to the documentation. I am really grateful that I got to work on an emerging CNCF project with such experienced engineers as my mentors. Working closing with such great engineers have taught me a lot about cloud native application development, testing and a lot more stuff. I am thankful to the Linux Foundation for conducting such an amazing open source program and doing it consistently over the years and I highly recommend it to everyone that at least applying for LFX if you are a student or somebody interested in FOSS, even the application process and contributions will teach you a lot about FOSS development\nA small thank you note to Linux Foundation I want to thank the entire Linux Foundation team, specially Nate Waddington and Sriji Ammanath. During this LFX period I also suffered a cyber fraud and the LF team specially Nate and Sriji were kind enough to pause my payments and patiently waited for the whole thing to be fixed. Thank you guys :)\nA small thank you note to my mentors I would also like to thank my mentors Fisher Xu and Shelley Bao Yue for being so helpful and so supportive throughout the mentorship. I am thankful that you gave me some time from your busy days and helped me out wherever I got stuck. Thank you mentors, you guys are awesome :)\n","permalink":"http://localhost:1313/blog/lfx/","summary":"\u003cp\u003eContributing to open source, specially to the Cloud Native Community Foundation (CNCF) has been one of the most effective ways for me to learn programming, understand how large codebases work and work with experienced engineers on large scale projects. It\u0026rsquo;s been a year for me as a contributor in the CNCF ecosystem and I also have recently completed my LFX (Linux Foundation Mentorship) program as a mentee. As I wrap up my LFX  program I want to share my journey in open source - from contributing to CNCF projects to getting into the prestigious LFX (Linux Foundation Mentorship) program and completing it successfully.\u003c/p\u003e","title":"My journey from a CNCF contributor to LFX mentee"},{"content":"When creating a website\u0026rsquo;s backend, one very important term we get to hear is JWT authentication. JWT authentication is one of the most popular ways of securing APIs. JWT stands for JSON Web Token and it is an open standard that defines a way for transmitting information between parties as a JSON object and that too securely. In this article, we will discuss JWT authentication and most importantly we will create an entire project for Fast and Efficient JWT authentication using Gin-Gonic, this will be a step-by-step project tutorial that will help you create a very fast and industry-level authentication API for your website or application\u0026rsquo;s backend.\nTable of Content:\nWhat is JWT authentication? Project Structure Prerequisites Step by Step Tutorial Final Results Output Conclusion What is JWT authentication? JWT stands for JSON Web Token and it is an open standard that defines a way for transmitting information between parties as a JSON object and that too securely.\nLet\u0026rsquo;s try to understand that by the help of an example. Consider a situation when we show up at a hotel and we walk up to the front desk and the receptionist says \u0026ldquo;hey, what can I do for you?\u0026rdquo;. I would say \u0026ldquo;Hi, my name is Shubham, I\u0026rsquo;m here for a conference, the sponsors are paying for my hotel\u0026rdquo;. The receptionist says \u0026ldquo;okay great! well I\u0026rsquo;m going to need to see a couple things\u0026rdquo;. Usually they\u0026rsquo;ll need to see my identification so to prove who I am and once that they have verified that I am the right person, they will issue me a key. And authentication works in a very similar way to this example.\nWith JWT authentication, we are making a request out to a server saying \u0026ldquo;Hey! here\u0026rsquo;s my username and password or sign-in token is this\u0026rdquo; and the website says \u0026ldquo;okay, let me check.\u0026rdquo; If my username and password is correct then that would give me a token. Now on subsequent requests of the server I don\u0026rsquo;t have to any longer include my username and password. I would just carry my token and check into the hotel (website), access to the Gym (data), I would have access to the pool(information) and only to my hotel room (account), I don\u0026rsquo;t have access to any other hotel rooms (other user\u0026rsquo;s account). This token is only authorized during the duration of my state so from check-in time to the checkout time. After that it is of no use. Now the hotel will also allow people without any verification to at least see the hotel, to roam in the public area around the hotel until you are coming inside the hotel, Similarly being an anonomous user you can interact with the website\u0026rsquo;s home page, landing page, etc.\nHere\u0026rsquo;s an example of a JWT :\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ .SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c Project Structure Here is the structure of the project. Make sure to create a similar folder structure in your workspace as well. In this structure we have 6 folders:\ncontrollers database helpers middleware models routes and create the respective files inside these folders.\nPrerequisites Go - Version 1.18+ Mongo DB Step by Step Tutorial Step 1. Let\u0026rsquo;s start the project by creating a module, the name of my module is \u0026ldquo;jwt\u0026rdquo; and my username is \u0026ldquo;1shubham7\u0026rdquo;, therefore I will initial my module by entering:\ngo mod init github.com/1shubham7/jwt This will create a go.mod file.\nStep 2. Create a main.go file and let\u0026rsquo;s create a web server in main.go. For that, add the following code in the file:\npackage main import( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;os\u0026#34; routes \u0026#34;github.com/1shubham7/jwt/routes\u0026#34; \u0026#34;github.com/joho/godotenv\u0026#34; \u0026#34;log\u0026#34; ) func main() { err := godotenv.Load(\u0026#34;.env\u0026#34;) if err != nil { log.Fatal(\u0026#34;Error locading the .env file\u0026#34;) } port := os.Getenv(\u0026#34;PORT\u0026#34;) if port == \u0026#34;\u0026#34; { port = \u0026#34;1111\u0026#34; } router := gin.New() router.Use(gin.Logger()) routes.AuthRoutes(router) routes.UserRoutes(router) // creating two APIs router.GET(\u0026#34;/api-1\u0026#34;, func(c *gin.Context){ c.JSON(200, gin.H{\u0026#34;success\u0026#34;:\u0026#34;Access granted for api-1\u0026#34;}) }) router.GET(\u0026#34;api-2\u0026#34;, func(c *gin.Context){ c.JSON(200, gin.H{\u0026#34;success\u0026#34;:\u0026#34;Access granted for api-2\u0026#34;}) }) router.Run(\u0026#34;:\u0026#34; + port) } \u0026lsquo;os\u0026rsquo; package to retrieve environment variables. we are using gin-gonic package to create a web server. Later we will create a routes package. AuthRoutes(), UserRoutes() are functions inside a file from routes package, we will create it later on.\nStep 3. Download the gin-gonic package:\ngo get github.com/gin-gonic/gin Step 4. Create a models folder and inside it create a userModel.go file. Enter the following code inside the userModel.go:\npackage models import ( \u0026#34;go.mongodb.org/mongo-driver/bson/primitive\u0026#34; \u0026#34;time\u0026#34; ) type User struct { ID primitive.ObjectID `bson:\u0026#34;id\u0026#34;` First_name *string `json:\u0026#34;first_name\u0026#34; validate:\u0026#34;required, min=2, max=100\u0026#34;` Last_name *string `json:\u0026#34;last_name\u0026#34; validate:\u0026#34;required, min=2, max=100\u0026#34;` Password *string `json:\u0026#34;password\u0026#34; validate:\u0026#34;required, min=6\u0026#34;` Email *string `json:\u0026#34;email\u0026#34; validate:\u0026#34;email, required\u0026#34;` //validate email means it should have an @ Phone *string `json:\u0026#34;phone\u0026#34; validate:\u0026#34;required\u0026#34;` Token *string `json:\u0026#34;token\u0026#34;` User_type *string `json:\u0026#34;user_type\u0026#34; validate:\u0026#34;required, eq=ADMIN|eq=USER\u0026#34;` Refresh_token *string `json:\u0026#34;refresh_token\u0026#34;` Created_at time.Time `json:\u0026#34;created_at\u0026#34;` Updated_at time.Time `json:\u0026#34;updated_at\u0026#34;` User_id string `json:\u0026#34;user_id\u0026#34;` } We have created a struct called User and have added necessary fields to the struct.\njson:\u0026quot;first_name\u0026quot; validate:\u0026quot;required, min=2, max=100\u0026quot; this are called field tags, these are used during decoding and encoding of go code to JSON and JSON to go. Here validate:\u0026ldquo;required, min=2, max=100\u0026rdquo; this is used for validate that the particular field must have minimum 2 characters and maximum 100 characters.\nStep 5. Create a database folder and inside it create a databaseConnection.go file, enter the following code inside it:\npackage database import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; \u0026#34;context\u0026#34; \u0026#34;github.com/joho/godotenv\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/mongo\u0026#34; \u0026#34;go/mongodb.org/mongo-driver/mongo/options\u0026#34; ) func DBinstance() *mongo.Client{ err := godotenv.Load(\u0026#34;.env\u0026#34;) if err != nil { log.Fatal(\u0026#34;Error locading the .env file\u0026#34;) } MongoDb := os.Getenv(\u0026#34;THE_MONGODB_URL\u0026#34;) client, err := mongo.NewClient(options.Client().ApplyURI(MongoDb)) if err != nil { log.Fatal(err) } ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() err = client.Connect(ctx) if err!=nil{ log.Fatal(err) } fmt.Println(\u0026#34;Connected to MongoDB!!\u0026#34;) return client } var Client *mongo.Client = DBinstance() func OpenCollection(client *mongo.Client, collectionName string) *mongo.Collection { var collection *mongo.Collection = client.Database(\u0026#34;cluster0\u0026#34;).Collection(collectionName) return collection } also make sure to download the \u0026lsquo;mongo\u0026rsquo; package:\ngo get go.mongodb.org/mongo-driver/mongo Here we are connecting your mongo database with the application.\nwe are using \u0026lsquo;godotenv\u0026rsquo; for loading environment variables that we will set in the .env file in main directory. The DBinstance Function, we take the value of \u0026ldquo;THE_MONGODB_URL\u0026rdquo; from the .env file (we will create that in the coming steps) and create a new mongoDB client using the value. \u0026lsquo;context\u0026rsquo; is used to have a timeout of 10 seconds. OpenCollection Function() takes client and collectionName as input and creates a collection for it.\nStep 6. For the routes, we will create two different files, authRouter and userRouter. authRouter includes \u0026lsquo;/signup\u0026rsquo; and \u0026lsquo;/login\u0026rsquo; . These will public to everyone so that they can authorize themselves. userRouter will not be public to everyone. It will include \u0026lsquo;/users\u0026rsquo; and \u0026lsquo;/users/:user_id\u0026rsquo;.\nCreate a folder called routes and add two files into it:\nuserRouter.go authRouter.go enter the following code into userRouter.go:\npackage routes import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; controllers \u0026#34;github.com/1shubham7/jwt/controllers\u0026#34; middleware \u0026#34;github.com/1shubham7/jwt/middleware\u0026#34; ) // user should not be able to use userRoute without the token func UserRoutes (incomingRoutes *gin.Engine) { incomingRoutes.Use(middleware.Authenticate()) // user routes are public routes but these must be authenticated, that // is why we have Authenticate() before these incomingRoutes.GET(\u0026#34;/users\u0026#34;, controllers.GetUsers()) incomingRoutes.GET(\u0026#34;users/:user_id\u0026#34;, controllers.GetUserById()) } Step 7. enter the following code into the authRouter.go:\npackage routes import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; controllers \u0026#34;github.com/1shubham7/jwt/controllers\u0026#34; ) // this is when the user has not signed up. userRouter is when the user has logged in // and has the token. func AuthRoutes(incomingRoutes *gin.Engine) { incomingRoutes.POST(\u0026#34;users/signup\u0026#34;, controllers.SignUp()) incomingRoutes.POST(\u0026#34;user/login\u0026#34;, controllers.Login()) } Step 8. create a folder called controllers and add a file called \u0026lsquo;userController.go\u0026rsquo; to it. enter the following code inside it.\npackage controllers import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;time\u0026#34; database \u0026#34;github.com/1shubham7/jwt/database\u0026#34; helper \u0026#34;github.com/1shubham7/jwt/helpers\u0026#34; models \u0026#34;github.com/1shubham7/jwt/models\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/go-playground/validator/v10\u0026#34; \u0026#34;golang.org/x/crypto/bcrypt\u0026#34; ) var userCollection *mongo.Collection = database.OpenCollection(database.Client, \u0026#34;user\u0026#34;) func Hashpassword() func VerifyPassword() func SignUp() func Login() func GetUsers() func GetUserById() The packages used are straight forward. database, helper and models are our own packages. \u0026#39;fmt\u0026#39; package is used for string formatting. \u0026#39;time\u0026#39; package is used for time representation \u0026#39;net/http\u0026#39; package is used for dealing with requests \u0026#39;strconv\u0026#39; package is used to convert string to int and vice versa. Then we have just created the outline function, we will complete those functions in the coming steps. Step 9. To keep environment variables in a separate file, we will create a .env file inside the main folder (root folder) and add the following code into it: PORT=1111 THE_MONGODB_URL=mongodb://localhost:27017/go-auth These two variables have already been used in our previous code.\nStep 10. Let\u0026rsquo;s create the GetUserById() function first. Enter the following code in GetUserById() function:\nfunc GetUserById() gin.HandlerFunc{ return func(c *gin.Context){ userId := c.Param(\u0026#34;user_id\u0026#34;) // we are taking the user_id given by the user in json // with the help of gin.context we can access the json data send by postman or curl or user if err := helper.MatchUserTypeToUserId(c, userId); err != nil{ //checking if the user in admin or not. // we will create that func in helper package. c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } var ctx, cancel = context.WithTimeout(context.Background(), 100*time.Second) var user models.User err := userCollection.FindOne(ctx, bson.M{\u0026#34;user_id\u0026#34;: userId}).Decode(\u0026amp;user) defer cancel() if err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } // if everything goes ok, pass the data of the user (UserModel.go) c.JSON(http.StatusOK, user) } } Step 11. Let\u0026rsquo;s create a folder called helpers and add a file called authHelper.go into it. Enter the following code into the authHelper.go :\npackage helpers import ( \u0026#34;errors\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func CheckUserType(c *gin.Context, userOrAdmin string) (err error) { userType := c.GetString(\u0026#34;user_type\u0026#34;) err = nil if userType != userOrAdmin { err = errors.New(\u0026#34;Not authorized to access the resource\u0026#34;) return err } return err } func MatchUserTypeToUserId(c *gin.Context, userId string) (err error) { // This is the match user function userType := c.GetString(\u0026#34;user_type\u0026#34;) uid := c.GetString(\u0026#34;uid\u0026#34;) err = nil // this means that user is USER not ADMIN and uid is not of the user. Because user can only access his id, // admin can access anyone\u0026#39;s id if userId == \u0026#34;USER\u0026#34; \u0026amp;\u0026amp; uid != userId { err = errors.New(\u0026#34;You are not authorized to access this user\u0026#34;) } err = CheckUserType(c, userType) return err } The MatchUserTypeToUserId() function just matches if the user is a Admin or just a user. We are using CheckUserType() function inside MatchUserTypeToUserId(), this is just checking if everything is fine (if the user_type we get from user is same as the userType variable.\nStep 12. We can now work on the SignUp() function of userController.go:\nfunc SignUp()gin.HandlerFunc{ return func(c *gin.Context){ var ctx, cancel = context.WithTimeout(context.Background(), 100*time.Second) var user models.User if err := c.BindJSON(\u0026amp;user); err!=nil{ c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } validationErr := validate.Struct(user) // this is used to validate, but what? see the User struct, and see those validate struct fields if validationErr != nil { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: validationErr.Error()}) return } // we are using count to help us validate. if you find documents with the user email already // then count would be more than 0, and we can then handle that err count, err := userCollection.CountDocuments(ctx, bson.M{\u0026#34;email\u0026#34;:user.Email}) defer cancel() if err != nil { log.Panic(err) c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;:\u0026#34;error occured while checking for the email\u0026#34;}) } password := HashPassword(*user.Password) user.Password = \u0026amp;password count, err = userCollection.CountDocuments(ctx, bson.M{\u0026#34;phone\u0026#34;:user.Phone}) defer cancel() if err != nil { log.Panic(err) c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;:\u0026#34;error occured while checking for the phone number\u0026#34;}) } if count \u0026gt; 0 { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;:\u0026#34;this email or phone number already exists\u0026#34;}) } // by \u0026#34;c.BindJSON(\u0026amp;user)\u0026#34; user already have the information from the website user user.Created_at, _ = time.Parse(time.RFC3339, time.Now().Format(time.RFC3339)) user.Updated_at, _ = time.Parse(time.RFC3339, time.Now().Format(time.RFC3339)) user.ID = primitive.NewObjectID() user.User_id = user.ID.Hex() token, refreshToken, _ := helper.GenerateAllTokens(*user.Email, *user.First_name, *user.Last_name, *user.User_type, *\u0026amp;user.User_id) // giving value that we generated to user user.Token = \u0026amp;token user.Refresh_token = \u0026amp;refreshToken // now let\u0026#39;s insert it to the database resultInsertionNumber, insertErr := userCollection.InsertOne(ctx, user) if insertErr != nil { msg := fmt.Sprintf(\u0026#34;User item was not created\u0026#34;) c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: msg}) return } defer cancel() c.JSON(http.StatusOK, resultInsertionNumber) } } We created a variable user of type User.\nvalidationErr is used to validate the struct tags, we already discussed this.\nWe are also using count variable to validate. As in if we find documents with the user email already then the count would be more than 0, and we can then handle that error (err)\nThen we are using \u0026rsquo;time.Parse(time.RFC3339, time.Now().Format(time.RFC3339))\u0026rsquo; to set the time for Created_at, Updated_at struct fields. When the user tries to sign up, the function SignUp() will run and that particular time will be stored in Created_at, Updated_at struct fields.\nThen we are creating tokens using GenerateAllTokens() function that we will create in tokenHelper.go file in the same package in the next step.\nWe also have a HashPassword() function that is doing nothing but hashing the user.password and replacing the user.password with the hashed password. We will also create that thing later.\nAnd then we are just inserting the data and the tokens etc. to the userCollection\nIf everything goes right, we will give StatusOK back.\nStep 13. create a file in helpers folder called \u0026rsquo;tokenHelper.go\u0026rsquo; and enter the following code inside it.\npackage helpers import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/1shubham7/jwt/database\u0026#34; jwt \u0026#34;github.com/dgrijalva/jwt-go\u0026#34; // golang driver for jwt \u0026#34;go.mongodb.org/mongo-driver/bson\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/bson/primitive\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/mongo\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/mongo/options\u0026#34; ) type SignedDetails struct { Email string First_name string Last_name string Uid string User_type string jwt.StandardClaims } var userCollection *mongo.Collection = database.OpenCollection(database.Client, \u0026#34;user\u0026#34;) // btw we sould have our secret key in .env for production var SECRET_KEY string = os.Getenv(\u0026#34;SECRET_KEY\u0026#34;) func GenerateAllTokens(email string, firstName string, lastName string, userType string, uid string) (signedToken string, signedRefreshToken string, err error){ claims := \u0026amp;SignedDetails{ Email : email, First_name: firstName, Last_name: lastName, Uid : uid, User_type: userType, StandardClaims: jwt.StandardClaims{ // setting the expiry time ExpiresAt: time.Now().Local().Add(time.Hour *time.Duration(120)).Unix(), }, } // refreshClaims is used to get a new token if th eprevious once is expired. refreshClaims := \u0026amp;SignedDetails{ StandardClaims: jwt.StandardClaims{ ExpiresAt: time.Now().Local().Add(time.Hour *time.Duration(172)).Unix(), }, } token ,err := jwt.NewWithClaims(jwt.SigningMethodHS256, claims).SignedString([]byte(SECRET_KEY)) refreshToken, err := jwt.NewWithClaims(jwt.SigningMethodHS256, refreshClaims).SignedString([]byte(SECRET_KEY)) if err!= nil{ log.Panic(err) return } return token, refreshToken, err } Also make sure to download the github.com/dgrijalva/jwt-go package:\ngo get github.com/dgrijalva/jwt-go Here we are using github.com/dgrijalva/jwt-go for generating tokens.\nWe are creating a struct called SignedDetails with field names required for generating tokens.\nwe are using NewWithClaims to generate new tokens and are giving the value to the claims and refreshClaims structs. claims has token for the first time users and refreshClaims has it when the user has to refresh the token. that is, they previously had a token which is now expired.\ntime.Now().Local().Add(time.Hour *time.Duration(120)).Unix() is being used for setting the expiry of the token.\nwe are then simply returning three things - token, refreshToken and err which we are using in the SignUp() function.\nStep 14. In the same file as SignUp() function, let\u0026rsquo;s create the HashPassword() function we talked about in step 9.\nfunc HashPassword(password string) string { hashed, err:=bcrypt.GenerateFromPassword([]byte(password), 14) if err!=nil{ log.Panic(err) } return string(hashed) } We are simply using the GenerateFromPassword() method from bcrypt package which is used to generate hashed passwords from the actual password.\nThis is important because we would not want and hacker to hack into our systems and steal all the passwords, also for the users privacy.\n[]byte (array of bytes) is simply string.\nStep 15. Let\u0026rsquo;s create the Login() function in \u0026lsquo;userController.go\u0026rsquo; and in later steps we can create the functions that Login() uses:\nfunc Login() gin.HandlerFunc{ return func(c *gin.Context) { var ctx, cancel = context.WithTimeout(context.Background(), 100*time.Second) var user models.User var foundUser models.User // giving the user data to user variable if err := c.BindJSON(\u0026amp;user); err != nil { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } // finding the user through email and if found, storing it in foundUser variable err := userCollection.FindOne(ctx, bson.M{\u0026#34;email\u0026#34;: user.Email}).Decode(\u0026amp;foundUser) defer cancel() if err!=nil{ c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: \u0026#34;email or password is incorrect\u0026#34;}) return } // we need pointer to acess the origina user and foundUser, // if we only pass user and foundUser, it will create a new instance of user and foundUser isPasswordValid, msg := VerifyPassword(*user.Password, *foundUser.Password) defer cancel() if isPasswordValid != true{ c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: msg}) return } if foundUser.Email == nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: \u0026#34;user not found\u0026#34;}) } token, refreshToken, _ := helper.GenerateAllTokens(*foundUser.Email, *foundUser.First_name, *foundUser.Last_name, *foundUser.User_type, foundUser.User_id) helper.UpdateAllTokens(token, refreshToken, foundUser.User_id) err = userCollection.FindOne(ctx, bson.M{\u0026#34;user_id\u0026#34;:foundUser.User_id}).Decode(\u0026amp;foundUser) if err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } c.JSON(http.StatusOK, foundUser) } } We are creating two variables of type User, these are user and Founduser. and giving the data from request to user.\nBy the help of \u0026lsquo;userCollection.FindOne(ctx, bson.M{\u0026ldquo;email\u0026rdquo;: user.Email}).Decode(\u0026amp;foundUser)\u0026rsquo; we are finding the user through his/her email and if found, storing it in foundUser variable.\nThen we are using VerifyPassword() function to verify the password and store it, remember that we are taking pointers as parameters in VerifyPassword(), if not, it would create a new instance of those in parameters rather than actually changing them.\nWe will create VerifyPassword() in the next step.\nThen we are simply using GenerateAllTokens() and UpdateAllTokens() to generate and update the token and refreshToken (which are basically tokens).\nAnd every step, we are all handling the errors.\nStep 16. Let\u0026rsquo;s create the VerifyPassword() function in the same file as Login() function:\nfunc VerifyPassword(userPassword, providedPassword string) (bool, string) { err := bcrypt.CompareHashAndPassword([]byte(providedPassword), []byte(userPassword)) check := true msg := \u0026#34;\u0026#34; if err != nil { check = false msg = fmt.Sprintf(\u0026#34;email or password is incorrect.\u0026#34;) } return check, msg } We are simply using the CompareHashAndPassword() method from bcrypt package which is used to compare hashed passwords. and returning a boolean value depening on the results.\n[]byte (array of bytes) is simply string, but []byte helps in comparing.\nStep 17. Let\u0026rsquo;s create the UpdateAllTokens() function in the \u0026rsquo;tokenHelper.go\u0026rsquo; file:\nfunc UpdateAllTokens(signedToken string, signedRefreshToken string, userId string){ var ctx, cancel = context.WithTimeout(context.Background(), 100*time.Second) var updateObj primitive.D updateObj = append(updateObj, bson.E{\u0026#34;token\u0026#34;, signedToken}) updateObj = append(updateObj, bson.E{\u0026#34;refresh_token\u0026#34;, signedRefreshToken}) Updated_at, _ := time.Parse(time.RFC3339, time.Now().Format(time.RFC3339)) updateObj = append(updateObj, bson.E{\u0026#34;updated_at\u0026#34;, Updated_at}) upsert := true filter := bson.M{\u0026#34;user_id\u0026#34;:userId} opt := options.UpdateOptions{ Upsert: \u0026amp;upsert, } _, err := userCollection.UpdateOne( ctx, filter, bson.D{ {\u0026#34;$set\u0026#34;, updateObj}, }, \u0026amp;opt, ) defer cancel() if err!=nil{ log.Panic(err) return } return } We are creating a variable called updateObj which is of type primitive.D. primitive.D type in MongoDB\u0026rsquo;s Go driver is a representation of a BSON document.\nAppend() is appending a key-value pair to updateObj each and every time its running.\nThen \u0026rsquo;time.Parse(time.RFC3339, time.Now().Format(time.RFC3339))\u0026rsquo; is used to update the current time (time when the updation happens and the function runs) to Updated_at.\nRest of the code block is performing update using the UpdateOne method of mongoDB collection.\nAt the last step we are also handling the error in case any error occurs.\nStep 18. Before continuing ahead, let\u0026rsquo;s download the go.mongodb.org/mongo-driver package:\ngo get go.mongodb.org/mongo-driver Step 19. Let\u0026rsquo;s now work on GetUserById() function. Remember that GetUserById() is for the users to access there own information, Admins can access all the users data, users can only access theirs.\nfunc GetUserById() gin.HandlerFunc{ return func(c *gin.Context){ userId := c.Param(\u0026#34;user_id\u0026#34;) // we are taking the user_id given by the user in json // with the help of gin.context we can access the json data send by postman or curl or user if err := helper.MatchUserTypeToUserId(c, userId); err != nil{ //checking if the user in admin or not. // we will create that func in helper package. c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } var ctx, cancel = context.WithTimeout(context.Background(), 100*time.Second) var user models.User err := userCollection.FindOne(ctx, bson.M{\u0026#34;user_id\u0026#34;: userId}).Decode(\u0026amp;user) defer cancel() if err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } // if everything goes ok, pass the data of the user (UserModel.go) c.JSON(http.StatusOK, user) } } Taking the user_id from the request and storing it in userId variable.\ncreating a variable called user of type User. then simply searching the user in our database with the help of user_id, if the user_id matches, we will store that persons information in user variable.\nIf everything goes fine, StatusOk\nwe are also handling the errors at every step.\nStep 20. Let\u0026rsquo;s now work on GetUsers() function. Remember that only the admin can access the GetUsers() function because it would include the data of all the users. Create a GetUsers() function in the same file as GetUserById(), Login() and SignUp() function:\nfunc GetUsers() gin.HandlerFunc{ return func(c *gin.Context){ if err := helper.CheckUserType(c, \u0026#34;ADMIN\u0026#34;); err != nil{ c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;:err.Error()}) return } var ctx, cancel = context.WithTimeout(context.Background(), 100*time.Second) // setting how many records you want per page. // we are taking the recodePerPage from c and converting it to int recordPerPage, err := strconv.Atoi(c.Query(\u0026#34;recordPerPage\u0026#34;)) // if error or recordPerPage is less than 1, by default we will have 9 records per page if recordPerPage\u0026lt;1||err != nil { recordPerPage = 9 } // this is just like page number page, err1 := strconv.Atoi(c.Query(\u0026#34;page\u0026#34;)) // we want to start with the page number 1 by default. if err1 !=nil || page \u0026lt; 1{ page = 1 } startIndex := (page - 1) * recordPerPage startIndex, err = strconv.Atoi(c.Query(\u0026#34;startIndex\u0026#34;)) matchStage := bson.D{{\u0026#34;$match\u0026#34;, bson.D{{}}, }} // group all the data based on id, and then count them using $sum. then // pushing all the data to the root. groupStage := bson.D{{\u0026#34;$group\u0026#34;, bson.D{{\u0026#34;_id\u0026#34;, bson.D{{\u0026#34;_id\u0026#34;, \u0026#34;null\u0026#34;}}}, {\u0026#34;total_count\u0026#34;, bson.D{{\u0026#34;$sum\u0026#34;, 1}}}, {\u0026#34;data\u0026#34;, bson.D{{\u0026#34;$push\u0026#34;, \u0026#34;$$ROOT\u0026#34;}}}, }}} // in project stage we deside which data should go to the user and which not. projectStage := bson.D{ {\u0026#34;$project\u0026#34;, bson.D{ {\u0026#34;_id\u0026#34;, 0}, {\u0026#34;total_count\u0026#34;, 1}, {\u0026#34;user_items\u0026#34;, bson.D{{\u0026#34;$slice\u0026#34;, []interface{}{\u0026#34;$data\u0026#34;, startIndex, recordPerPage}}}}, }}, } result, err := userCollection.Aggregate(ctx, mongo.Pipeline{ matchStage, groupStage, projectStage}) defer cancel() if err != nil{ c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;:\u0026#34;error occured while listing user items\u0026#34;}) } // creating a slice called allUser and giving the result value var allUsers []bson.M if err := result.All(ctx, \u0026amp;allUsers); err != nil { log.Fatal(err) } c.JSON(http.StatusOK, allUsers[0]) } } Firstly, we will check if the request is coming from the admin or not, we do that by the help of CheckUserType() we created in previous steps.\nThen we are setting how many records you want per page.\nWe can do that by taking the recodePerPage from the request and converting it to int, this is done by srtconv.\nIf any error occurs in setting recordPerPage or recordPerPage is less than 1, by default we will have 9 records per page\nSimilarly we are taking the page number in variable \u0026lsquo;page\u0026rsquo;.\nBy default we will have page number as 1 and 9 recordPerPage.\nThen we created three stages (matchStage, groupStage, projectStage).\nThen we are setting these three stages in our Mongo pipeline using Aggregate() function\nAlso we are handling errors are every step.\nStep 21. Our \u0026lsquo;userController.go\u0026rsquo; is now ready, this is how the \u0026lsquo;userController.go\u0026rsquo; file looks like after completion:\npackage controllers import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;time\u0026#34; database \u0026#34;github.com/1shubham7/jwt/database\u0026#34; helper \u0026#34;github.com/1shubham7/jwt/helpers\u0026#34; models \u0026#34;github.com/1shubham7/jwt/models\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/go-playground/validator/v10\u0026#34; \u0026#34;golang.org/x/crypto/bcrypt\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/bson\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/bson/primitive\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/mongo\u0026#34; ) var userCollection *mongo.Collection = database.OpenCollection(database.Client, \u0026#34;user\u0026#34;) var validate = validator.New() func HashPassword(password string) string { hashed, err:=bcrypt.GenerateFromPassword([]byte(password), 14) if err!=nil{ log.Panic(err) } return string(hashed) } func VerifyPassword(userPassword, providedPassword string) (bool, string) { err := bcrypt.CompareHashAndPassword([]byte(providedPassword), []byte(userPassword)) check := true msg := \u0026#34;\u0026#34; if err != nil { check = false msg = fmt.Sprintf(\u0026#34;email or password is incorrect.\u0026#34;) } return check, msg } func SignUp()gin.HandlerFunc{ return func(c *gin.Context){ var ctx, cancel = context.WithTimeout(context.Background(), 100*time.Second) var user models.User if err := c.BindJSON(\u0026amp;user); err!=nil{ c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } validationErr := validate.Struct(user) // this is used to validate, but what? see the User struct, and see those validate struct fields if validationErr != nil { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: validationErr.Error()}) return } // we are using count to help us validate. if you find documents with the user email already // then count would be more than 0, and we can then handle that err count, err := userCollection.CountDocuments(ctx, bson.M{\u0026#34;email\u0026#34;:user.Email}) defer cancel() if err != nil { log.Panic(err) c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;:\u0026#34;error occured while checking for the email\u0026#34;}) } password := HashPassword(*user.Password) user.Password = \u0026amp;password count, err = userCollection.CountDocuments(ctx, bson.M{\u0026#34;phone\u0026#34;:user.Phone}) defer cancel() if err != nil { log.Panic(err) c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;:\u0026#34;error occured while checking for the phone number\u0026#34;}) } if count \u0026gt; 0 { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;:\u0026#34;this email or phone number already exists\u0026#34;}) } // by \u0026#34;c.BindJSON(\u0026amp;user)\u0026#34; user already have the information from the website user user.Created_at, _ = time.Parse(time.RFC3339, time.Now().Format(time.RFC3339)) user.Updated_at, _ = time.Parse(time.RFC3339, time.Now().Format(time.RFC3339)) user.ID = primitive.NewObjectID() user.User_id = user.ID.Hex() token, refreshToken, _ := helper.GenerateAllTokens(*user.Email, *user.First_name, *user.Last_name, *user.User_type, *\u0026amp;user.User_id) // giving value that we generated to user user.Token = \u0026amp;token user.Refresh_token = \u0026amp;refreshToken // now let\u0026#39;s insert it to the database resultInsertionNumber, insertErr := userCollection.InsertOne(ctx, user) if insertErr != nil { msg := fmt.Sprintf(\u0026#34;User item was not created\u0026#34;) c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: msg}) return } defer cancel() c.JSON(http.StatusOK, resultInsertionNumber) } } func Login() gin.HandlerFunc{ return func(c *gin.Context) { var ctx, cancel = context.WithTimeout(context.Background(), 100*time.Second) var user models.User var foundUser models.User // giving the user data to user variable if err := c.BindJSON(\u0026amp;user); err != nil { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } // finding the user through email and if found, storing it in foundUser variable err := userCollection.FindOne(ctx, bson.M{\u0026#34;email\u0026#34;: user.Email}).Decode(\u0026amp;foundUser) defer cancel() if err!=nil{ c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: \u0026#34;email or password is incorrect\u0026#34;}) return } // we need pointer to acess the origina user and foundUser, // if we only pass user and foundUser, it will create a new instance of user and foundUser isPasswordValid, msg := VerifyPassword(*user.Password, *foundUser.Password) defer cancel() if isPasswordValid != true{ c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: msg}) return } if foundUser.Email == nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: \u0026#34;user not found\u0026#34;}) } token, refreshToken, _ := helper.GenerateAllTokens(*foundUser.Email, *foundUser.First_name, *foundUser.Last_name, *foundUser.User_type, foundUser.User_id) helper.UpdateAllTokens(token, refreshToken, foundUser.User_id) err = userCollection.FindOne(ctx, bson.M{\u0026#34;user_id\u0026#34;:foundUser.User_id}).Decode(\u0026amp;foundUser) if err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } c.JSON(http.StatusOK, foundUser) } } // GetUsers can only be accessed by the admin. func GetUsers() gin.HandlerFunc{ return func(c *gin.Context){ if err := helper.CheckUserType(c, \u0026#34;ADMIN\u0026#34;); err != nil{ c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;:err.Error()}) return } var ctx, cancel = context.WithTimeout(context.Background(), 100*time.Second) // setting how many records you want per page. // we are taking the recodePerPage from c and converting it to int recordPerPage, err := strconv.Atoi(c.Query(\u0026#34;recordPerPage\u0026#34;)) // if error or recordPerPage is less than 1, by default we will have 9 records per page if recordPerPage\u0026lt;1||err != nil { recordPerPage = 9 } // this is just like page number page, err1 := strconv.Atoi(c.Query(\u0026#34;page\u0026#34;)) // we want to start with the page number 1 by default. if err1 !=nil || page \u0026lt; 1{ page = 1 } startIndex := (page - 1) * recordPerPage startIndex, err = strconv.Atoi(c.Query(\u0026#34;startIndex\u0026#34;)) matchStage := bson.D{{\u0026#34;$match\u0026#34;, bson.D{{}}, }} // group all the data based on id, and then count them using $sum. then // pushing all the data to the root. groupStage := bson.D{{\u0026#34;$group\u0026#34;, bson.D{{\u0026#34;_id\u0026#34;, bson.D{{\u0026#34;_id\u0026#34;, \u0026#34;null\u0026#34;}}}, {\u0026#34;total_count\u0026#34;, bson.D{{\u0026#34;$sum\u0026#34;, 1}}}, {\u0026#34;data\u0026#34;, bson.D{{\u0026#34;$push\u0026#34;, \u0026#34;$$ROOT\u0026#34;}}}, }}} // in project stage we deside which data should go to the user and which not. projectStage := bson.D{ {\u0026#34;$project\u0026#34;, bson.D{ {\u0026#34;_id\u0026#34;, 0}, {\u0026#34;total_count\u0026#34;, 1}, {\u0026#34;user_items\u0026#34;, bson.D{{\u0026#34;$slice\u0026#34;, []interface{}{\u0026#34;$data\u0026#34;, startIndex, recordPerPage}}}}, }}, } result, err := userCollection.Aggregate(ctx, mongo.Pipeline{ matchStage, groupStage, projectStage}) defer cancel() if err != nil{ c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;:\u0026#34;error occured while listing user items\u0026#34;}) } // creating a slice called allUser and giving the result value var allUsers []bson.M if err := result.All(ctx, \u0026amp;allUsers); err != nil { log.Fatal(err) } c.JSON(http.StatusOK, allUsers[0]) } } func GetUserById() gin.HandlerFunc{ return func(c *gin.Context){ userId := c.Param(\u0026#34;user_id\u0026#34;) // we are taking the user_id given by the user in json // with the help of gin.context we can access the json data send by postman or curl or user if err := helper.MatchUserTypeToUserId(c, userId); err != nil{ //checking if the user in admin or not. // we will create that func in helper package. c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } var ctx, cancel = context.WithTimeout(context.Background(), 100*time.Second) var user models.User err := userCollection.FindOne(ctx, bson.M{\u0026#34;user_id\u0026#34;: userId}).Decode(\u0026amp;user) defer cancel() if err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } // if everything goes ok, pass the data of the user (UserModel.go) c.JSON(http.StatusOK, user) } } Step 22. Now we can work on the authentication part. For that we will create a authenticate middleware. Create a folder called \u0026lsquo;middleware\u0026rsquo; and create a file inside it called \u0026lsquo;authMiddleware.go\u0026rsquo;. Enter the following code in the file:\npackage middleware import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;fmt\u0026#34; helpers \u0026#34;github.com/1shubham7/jwt/helpers\u0026#34; ) func Authenticate() gin.HandlerFunc{ return func(c *gin.Context) { clientToken := c.Request.Header.Get(\u0026#34;token\u0026#34;) if clientToken == \u0026#34;\u0026#34; { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;:fmt.Sprintf(\u0026#34;No Authorization header provided\u0026#34;)}) c.Abort() return } claims, err := helpers.ValidateToken(clientToken) if err !=\u0026#34;\u0026#34; { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;:err}) c.Abort() return } c.Set(\u0026#34;email\u0026#34;, claims.Email) c.Set(\u0026#34;first_name\u0026#34;, claims.First_name) c.Set(\u0026#34;last_name\u0026#34;, claims.Last_name) c.Set(\u0026#34;uid\u0026#34;, claims.Uid) c.Set(\u0026#34;user_type\u0026#34;, claims.User_type) c.Next() } } Step 23. Now let\u0026rsquo;s create ValidateToken() function, we will create this function in the \u0026rsquo;tokenHelper.go\u0026rsquo;:\nfunc ValidateToken(signedToken string) (claims *SignedDetails, msg string) { // this function is basically returning the token token, err := jwt.ParseWithClaims( signedToken, \u0026amp;SignedDetails{}, func(token *jwt.Token)(interface{}, error){ return []byte(SECRET_KEY), nil }, ) if err != nil { msg = err.Error() return } // checking if the token is correct or not claims, ok := token.Claims.(*SignedDetails) if !ok { msg = fmt.Sprintf(\u0026#34;the token is invalid\u0026#34;) msg = err.Error() return } // if the token is expired, give error message if claims.ExpiresAt \u0026lt; time.Now().Local().Unix(){ msg = fmt.Sprintf(\u0026#34;token has been expired\u0026#34;) msg = err.Error() return } return claims, msg } ValidateToken takes signedToken and returns SignedDetails of that along with an error message. \u0026quot;\u0026quot; if there is no error.\nParseWithClaims() function is being used to get us the token and store it in a variable called token.\nThen we are checking if the token is correct or not using the Claims method on token. And we are storing the result in claims variable.\nThen we are checking if the token has expired using ExpiresAt() function, if current time is greater than the ExpiresAt time, it would have expired.\nAnd then simply return the claims variable as well as the message.\nStep 24. We are mostly done now, let\u0026rsquo;s do \u0026lsquo;go mod tidy\u0026rsquo;, this command checks in your go.mod file, it deletes all the packages/dependencies that we installed but are not using and downloads any dependencies that we are using but haven\u0026rsquo;t downloaded yet.\ngo mod tidy Output With that our JWT authentication project is ready, to finally run the application enter the following command in the terminal:\ngo run main.go You will get a similar output:\nThis will get the server up and running, and you can use curl or Postman API for sending request and receiving response. Or you can simply integrate this API with an frontend framework. And with that, our authentication API is ready, pat yourself on the back!\nConclusion In this article we discussed one of the fastest ways to create JWT authentication, we used Gin-Gonic framework for our project. This is not your \u0026ldquo;just another authentication API\u0026rdquo;. Gin is 300% faster than NodeJS, that makes this authentication API really fast and efficient. The project structure we are using is also an industry level project structure. You can make further changes like storing the SECRET_KEY in the .env file and a lot more to make this API better. You can also find the source code for this project here - 1Shubham7/go-jwt-token.\nMake sure to follow all the steps in order to create the project and add some more functionality and just play with the code to understand it better. The best way to learn authentication is to create your own projects.\n","permalink":"http://localhost:1313/blog/jwt/","summary":"\u003cp\u003eWhen creating a website\u0026rsquo;s backend, one very important term we get to hear is JWT authentication. JWT authentication is one of the most popular ways of securing APIs. JWT stands for JSON Web Token and it is an open standard that defines a way for transmitting information between parties as a JSON object and that too securely. In this article, we will discuss JWT authentication and most importantly we will create an entire project for Fast and Efficient JWT authentication using Gin-Gonic, this will be a step-by-step project tutorial that will help you create a very fast and industry-level authentication API for your website or application\u0026rsquo;s backend.\u003c/p\u003e","title":"Building a Golang backend system with JWT authentication"},{"content":"About the internship GeeksforGeeks is a leading platform that provides computer science resources and coding challenges for programmers and technology enthusiasts, along with interview and exam preparations for upcoming aspirants. With a strong emphasis on enhancing coding skills and knowledge, it has become a trusted destination for over 12 million plus registered users worldwide. The platform offers a vast collection of tutorials, practice problems, interview tutorials, articles, and courses, covering various domains of computer science.\nI worked as a GeeksforGeeks Technical writing intern for a duration of 6 months. In this period I wrote multiple tutorials on Kubernetes, Docker, Learnings:\nWorked on my technical writing skills and learn a lot about how to write articles and tutorials. Created tutorials on Docker, Kubernetes, and web design. To teach is to learn twice. I wrote multiple tutorials on Kubernetes, Docker and Web design, and by working on these tutorials I got in depth knowledge of the concepts. ","permalink":"http://localhost:1313/experience/gfg/","summary":"\u003ch2 id=\"about-the-internship\"\u003eAbout the internship\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGeeksforGeeks\u003c/strong\u003e is a leading platform that provides computer science resources and coding challenges for programmers and technology enthusiasts, along with interview and exam preparations for upcoming aspirants. With a strong emphasis on enhancing coding skills and knowledge, it has become a trusted destination for over 12 million plus registered users worldwide. The platform offers a vast collection of tutorials, practice problems, interview tutorials, articles, and courses, covering various domains of computer science.\u003c/p\u003e","title":"GeeksforGeeks Technical Writer"},{"content":"CNCF ORAS CNCF is the open source, vendor-neutral hub of cloud native computing, hosting projects like Kubernetes and Prometheus to make cloud native universal and sustainable. ORAS is a sandbox project under CNCF.\nORAS is the de facto tool for working with OCI Artifacts. It treats media types as a critical piece of the puzzle. Container images are never assumed to be the artifact in question. ORAS provides CLI and client libraries to distribute artifacts across OCI-compliant registries.\nI contributed to 2 repositories of CNCF ORAS: oras and oras-www.\nLearnings Worked and connected with highly skilled developers. Worked with the documentation part of the website. Worked on the Golang part of the project and had 6 PRs merged into the project as of now (10th Aug 2023) Learned about how to navigate a professional codebase and how to contribute to it. ","permalink":"http://localhost:1313/experience/cncf-oras/","summary":"\u003ch2 id=\"cncf-oras\"\u003eCNCF ORAS\u003c/h2\u003e\n\u003cp\u003eCNCF is the open source, vendor-neutral hub of cloud native computing, hosting projects like Kubernetes and Prometheus to make cloud native universal and sustainable. ORAS is a sandbox project under CNCF.\u003c/p\u003e\n\u003cp\u003eORAS is the de facto tool for working with OCI Artifacts. It treats media types as a critical piece of the puzzle. Container images are never assumed to be the artifact in question. ORAS provides CLI and client libraries to distribute artifacts across OCI-compliant registries.\u003c/p\u003e","title":"CNCF ORAS Contributor"},{"content":"CNCF Zero to Merge Incubator CNCF (Cloud Native Community Foundation) Zero to merge is an Incubator program that help you work with CNCF projects, understand large and complex codebases and contribute to them. I was the part of the first cohort of CNCF Zero to merge where I learned about how to get started with contributing to CNCF projects. It was a one month long program where we started with introduction to CNCF and gradually progressed towards how to make contributions to various CNCF projects.\nLearning: Learned in depth about the CNCF Landscape. Enhanced my understanding of how to approach open-source projects, specifically CNCF Projects. Learned the correct approach for raising issues, creating PRs and participating in Discussions. ","permalink":"http://localhost:1313/experience/cncf-zero-to-merge/","summary":"\u003ch2 id=\"cncf-zero-to-merge-incubator\"\u003eCNCF Zero to Merge Incubator\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eCNCF (Cloud Native Community Foundation)\u003c/strong\u003e Zero to merge is an Incubator program that help you work with CNCF projects, understand large and complex codebases and contribute to them. I was the part of the first cohort of CNCF Zero to merge where I learned about how to get started with contributing to CNCF projects. It was a one month long program where we started with introduction to CNCF and gradually progressed towards how to make contributions to various CNCF projects.\u003c/p\u003e","title":"CNCF Zero to Merge"},{"content":"Hooman Digital We at Hooman Digital specialize in creating engaging and user-friendly designs for digital interfaces, as well as developing fully functional and responsive websites. Hooman Digital works with technologies around web development, Artificial Intelligence, and web3. Visit the Hooman Digital official website at hooman digital to know more.\nWorked as a Hugo expert with Hooman Digital. Worked with a team of developers to create static websites, transition Hugo + SCSS website to Tailwind and enhance functionality of multiple websites.\nCreated fast and optimised website for clients under Hooman Digital.\nLearnings Learned how to work in a remote start-up. How to work under a team of developers with different tech stack. Improved my understanding of Hugo and static site generation. Learned how to understand and work with complex codebases. Learned how to create and deploy professional applications with complex codebases. ","permalink":"http://localhost:1313/experience/hooman/","summary":"\u003ch2 id=\"hooman-digital\"\u003eHooman Digital\u003c/h2\u003e\n\u003cp\u003eWe at \u003cstrong\u003eHooman Digital\u003c/strong\u003e specialize in creating engaging and user-friendly designs for digital interfaces, as well as developing fully functional and responsive websites. Hooman Digital works with technologies around web development, Artificial Intelligence, and web3. Visit the Hooman Digital official website at hooman digital to know more.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eWorked as a Hugo expert with Hooman Digital. Worked with a team of developers to create static websites, transition Hugo + SCSS website to Tailwind and enhance functionality of multiple websites.\u003c/p\u003e","title":"Hugo Expert Intern"},{"content":"\u0026ldquo;Girlscript Summer Of Code is a three-month-long Open-Source Program conducted every summer by the Girlscript Foundation. With constant efforts, participants contribute to numerous projects under the extreme guidance of skilled mentors over these months. With such exposure, students begin to contribute to real-world projects from the comfort of their homes. GirlScript Summer Of Code has witnessed active participation over the years, and the 2023 edition aims to carry the legacy with a promising impact.\u0026rdquo;\nGirlscript Summer of Code was my first exposure to larger code bases. I was ranked under top 100 contributors out of 17,000 + contributors from 14+ countries.\nLearnings Learned how to contribute to larger code bases. Contributed to more than x projects under the domain of Web Dev, AI and web3. Merged x PR in projects ranging from Web development to Artificial Intelligence. ","permalink":"http://localhost:1313/experience/gssoc/","summary":"\u003cp\u003e\u0026ldquo;\u003cstrong\u003eGirlscript Summer Of Code\u003c/strong\u003e is a three-month-long Open-Source Program conducted every summer by the Girlscript Foundation. With constant efforts, participants contribute to numerous projects under the extreme guidance of skilled mentors over these months. With such exposure, students begin to contribute to real-world projects from the comfort of their homes. GirlScript Summer Of Code has witnessed active participation over the years, and the 2023 edition aims to carry the legacy with a promising impact.\u0026rdquo;\u003c/p\u003e","title":"GirlScript Summer of Code top 100 contributors"},{"content":"Tech Blogs on Hashnode Hashnode is a free developer blogging platform that allows you to publish articles on your own domain and helps you stay connected with a global developer community.\nLearnings Wrote about 20+ blogs on topics from web dev to DevOps and open source. Learned how to write technical blogs. Learned how to create simple and understandable tutorials. Got to know about how much a good documentation can help new comers to learn about the technology. ","permalink":"http://localhost:1313/experience/hashnode/","summary":"\u003ch2 id=\"tech-blogs-on-hashnode\"\u003eTech Blogs on Hashnode\u003c/h2\u003e\n\u003cp\u003eHashnode is a free developer blogging platform that allows you to publish articles on your own domain and helps you stay connected with a global developer community.\u003c/p\u003e\n\u003ch2 id=\"learnings\"\u003eLearnings\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eWrote about 20+ blogs on topics from web dev to DevOps and open source.\u003c/li\u003e\n\u003cli\u003eLearned how to write technical blogs.\u003c/li\u003e\n\u003cli\u003eLearned how to create simple and understandable tutorials.\u003c/li\u003e\n\u003cli\u003eGot to know about how much a good documentation can help new comers to learn about the technology.\u003c/li\u003e\n\u003c/ul\u003e","title":"Technical Blogs on Hashnode"},{"content":"Namespaces are Kubernetes Objects used to divide a Cluster into smaller organized sub clusters as per different needs. Namespaces enables users to isolate group of resources within a cluster, this is useful when multiple teams are working on a shared cluster and each team is independent of others. In this article, we will discuss how to change or switch namespaces in a Kubernetes cluster, but before that let\u0026rsquo;s start with the basics.\nKubernetes Namespaces Namespaces are Kubernetes Objects used to divide a Cluster into smaller organized sub clusters as per different needs. Namespaces enables users to isolate group of resources within a cluster, this is useful when multiple teams are working on a shared cluster and each team is independent of others. In simpler words, Namespaces are used to organize resources. You can have multiple Namespaces in a Cluster And these Namespaces are kind of virtual Clusters of their own. Within a Kubernetes Namespace, resources must have unique names, but across different Namespaces, you can have resources with the same name.\nWhy do we need Namespaces Now Let\u0026rsquo;s first discuss what is the need for Namespaces. when should you create them? And how you should use it?\nStructuring Resources in Groups The biggest use case of creating your Namespaces is this that instead of cluttering the Default Namespace with various components, Namespaces allow grouping resources logically. For Example, you can have a database Namespace where you deploy your database and all its required resources and you can have a Monitoring Namespace where you deploy monitoring related stuff.\nPreventing Conflicts between Multiple Teams When multiple teams share a cluster, Namespaces prevent conflicts by isolating team deployments. This ensures that each team can work independently without interfering with others. For example, one team deploys an application which is called \u0026ldquo;my-app-deployment\u0026rdquo; and that deployment has its certain configuration. Now if another team had a Deployment that accidentally had the same name but a different configuration and they created that Deployment or they applied it, they would overwrite the first team\u0026rsquo;s Deployment. But with different namespaces assigned to different teams, they can have same Deployment name without any conflicts.\nResource Sharing\nLets say you have one Cluster and you want to host both Staging and Development environment in the same Cluster. and the reason for that is if you\u0026rsquo;re using Nginx Controller or Elastic Stack for logging, you can deploy it in one Cluster and use it for both environments. In that way you don\u0026rsquo;t have to deploy these common resources twice in two different clusters. So now the staging can use both resources as well as the development environment.\nBlue/Green Deployment Namespaces facilitate Blue/Green Deployment by allowing different versions of an application to coexist in the same cluster, utilizing shared resources efficiently. Which means that in the same cluster you can have two different versions of Production - The one that is in production now and another one that is going to be the next production version.\nAccess Control and Resource Limitation Another reason for using Namespaces is to restrict access and resource consumption in multi-team environments. With each team having its Namespace, access can be limited to only their Namespace, preventing accidental interference with other teams\u0026rsquo; work. This ensures a secure and isolated environment for each team. Additionally, resource quotas per Namespace prevent one team from consuming excessive resources, ensuring fair resource allocation across the cluster.\nDefault Namespaces Default Namespaces are the four Namespaces that are present by default in any Kubernetes Cluster. To see these in your Namespaces you can simply enter this command:\nkubectl get namespaces And you will see that we have 4 default namespaces:\nScreenshot-2023-12-03-173805\nBefore learning about the 4 default Namespaces, let\u0026rsquo;s first briefly discuss kubernetes dashboard namespace, which is a Namespace that comes built-in with Minikube.\nkubernetes dashboard namespace: \u0026ldquo;kubernetes dashboard namespace\u0026rdquo; is shipped automatically in Minikube. So it is specific to Minikube installation. We will not have this namespace in a standard Cluster.\nkube-system kube-system is the namespace that includes objects created by the Kubernetes system. The components that are deployed in this Namespace are the system processes - they are from Master managing processes or Kubectl etc. kube-system Namespace is not meant for our (developer\u0026rsquo;s) use. so we do not have to create anything or modify anything in this namespace.\nkube-public kube-public contains the publicly accessible data. It has a config map that contains the Cluster information which is accessible even without authentication.\nyou can simply type:\nkubectl cluster-info You will get the data stored in kube-public namespace.\nScreenshot-2023-12-03-191002\nkube-node-lease kube-node-lease Namespace is a new addition to Kubernetes. The purpose of this namespace is that it holds information about the heartbeats of Nodes. So each Node basically gets its own lease object in the Namespace. This object contains the information about that nodes availability.\ndefault default Namespace is the Namespace that we use in order to create the resources when we create a Namespace in the beginning.\nHow to Change Namespace in Kubernetes - Tutorial Step 1: Starting a Kubernetes cluster You can skip this step if you already have a Kubernetes cluster running on your machine. But in case you don\u0026rsquo;t have a cluster running, enter the following command to create Kubernetes locally using Minikube:\nminikube start Screenshot-2024-03-27-085106\nStep 2. Creating Namespaces\nLet us create a Namespace so that we can later switch between the default and new namespace, before that let us check the list of namespaces we have currently in our cluster by entering the following command:\nkubectl get ns and you will get the same results:\nScreenshot-2024-03-27-013427\n\u0026ldquo;default\u0026rdquo; is the default namespace by Kubernetes and rest of the namespaces are created by Minikube. To create a namespace called namespace-one, enter the following command:\nkubectl create namespace namespace-one Step 3. Switching between namespaces\nNow that we have a new namespace created, let us learn how to switch between them. By default the currently active namespace is \u0026ldquo;default\u0026rdquo; namespace. You can check the currently active namespace by the following command:\nkubectl config view --minify | grep namespace You can change or switch Namespaces in Kubernetes using the following command:\nkubectl config set-context --current --namespace=namespace-one And now the namespace-one will be the currently active namespace. And is we check the currently active namespace:\nkubectl config view --minify | grep namespace You will see namespace-one as the currently active namespace:\nScreenshot-2024-03-27-083757\nThis is how you can change namespace or switch between namespace in a Kubernetes Cluster. Pat yourself on the back because we have just learned how to switch between Namespaces in a Kubernetes cluster.\nOperations on Kubernetes Namespaces To Create a Kubernetes Namespace enter the following command: kubectl create namespace \u0026lt;namespace-name\u0026gt; To List Namespaces in the Cluster enter the following command: kubectl get namespaces To Delete a Namespace enter the following command: kubectl delete namespace \u0026lt;namespace-name\u0026gt; To View Namespace Detail enter the following command: kubectl describe namespace \u0026lt;namespace-name\u0026gt; To Set Current Namespace enter the following command: kubectl config set-context --current --namespace=\u0026lt;namespace-name\u0026gt; To Edit a Namespace enter the following command: kubectl edit namespace \u0026lt;namespace-name\u0026gt; To List Pods in a Namespace enter the following command: kubectl get pods --namespace=\u0026lt;namespace-name\u0026gt; To List Services in a Namespace enter the following command: kubectl get services --namespace=\u0026lt;namespace-name\u0026gt; To Apply a Resource Configuration to a Namespace enter the following command: kubectl apply -f \u0026lt;resource-config.yaml\u0026gt; --namespace=\u0026lt;namespace-name\u0026gt; To Export Namespace Configuration enter the following command: kubectl get namespace \u0026lt;namespace-name\u0026gt; -o yaml \u0026gt; namespace-config.yaml To Import Namespace Configuration enter the following command: kubectl apply -f namespace-config.yaml Conclusion Kubernetes Namespaces are a way to group resources in a Kubernetes Cluster. In this article, we discussed how Namespaces work, What are the default Namespaces present already in a Kubernetes Cluster. From need of Namespaces to it\u0026rsquo;s characteristics, we discussed everything. Do follow the tutorial and try creating resources in a Namespace by your own because that\u0026rsquo;s the best way to learn. Make sure to learn more about kubectx and its features. We hope that this article helped you improve your understanding about Namespaces and you learned something valuable out of it.\nKubernetes Namespaces - FAQ\u0026rsquo;s What are the out of the box Namespaces provided by Kubernetes? There are three out of the box Namespaces provided by Kubernetes:\ndefault kube-system kube-public How to change Namespace in Kubernetes? You can change or switch Namespaces in Kubernetes using the following command:\nkubectl config set-context --current --namespace=namespace-one How do I export Namespace configuration? You can export the Namespace Configuration in Kubernetes by entering the following command:\nkubectl get namespace \u0026lt;namespace-name\u0026gt; -o yaml \u0026gt; namespace-config.yaml What is the purpose of a namespace? Purpose of Namespaces is to group resources of a Kubernetes Cluster in order to manage them in efficient manner.\nWhat are some unique characteristics of Namespaces? Some unique characteristics of Namespaces are as following:\nYou can not access most of the resources from another namespace. Service can be shared across Namespaces Volumes and Nodes cannot be created within a Namespace ","permalink":"http://localhost:1313/blog/namespaces/","summary":"\u003cp\u003eNamespaces are Kubernetes Objects used to divide a Cluster into smaller organized sub clusters as per different needs. Namespaces enables users to isolate group of resources within a cluster, this is useful when multiple teams are working on a shared cluster and each team is independent of others. In this article, we will discuss how to change or switch namespaces in a Kubernetes cluster, but before that let\u0026rsquo;s start with the basics.\u003c/p\u003e","title":"Beginners guide to Kubernetes namespaces"},{"content":"Kyverno is a dynamic admission controller that can be used to govern and enforce how a Kubernetes cluster is used.\nAdmission Controllers Let\u0026rsquo;s first start with admission controllers since Kyverno is an admission controller. In Kubernetes admission controllers are used to intercept requests going to the API servier and one of the two or both the things with them - mutate or/and validate. Kubernetes comes up with built-in admission controllers. The issue with these built-in admission controllers is that they are highly specific and different organizations may have different use cases with admission contollers not being fulfilled by these built-in admission controllers, therefore, for meeting these requirements we have dynamic admission controllers that can be developed as extensions and can be run as webhooks configured at runtime.\nAdmission control phases\nThe primary job of a admission controller is to validate the resources. If a resource is passed to an admission controller it will tell if it is allowed or not. And that is all what an admission controller does. But as we said Kyverno is more than just an admission controller. Kyverno acts as a admission controller and validates the resources but it also can change the resources transparently.\nIn this beginner friendly article, we will discuss what Kyverno is and how it works on theory, but before that let\u0026rsquo;s have a look at some basic terms:\nCustom resources Custom resources are the extensions of the Kubernetes API that are not available in the default Kubernetes installation. These are like additional stuff or customization we can add on Kubernetes. However, many Kubernetes core functions are now build using custom resources, this make Kubernetes very modular. A Kyverno policy is a standard Kubernetes custom resource.\nCustom Controllers The custom resources lets us store and retrieve structured data. But when we combine a custom resource with a custom controller, it provides us a declarative API. Custom controllers are used to encode domain knowledge for specific applications into an extension of Kubernetes API.\nAdmission Controllers In simplest terms, If a resource is passed to an admission controller it will tell if it is allowed or not. To explain further, the Kubernetes official documentation defines admission controller as - \u0026ldquo;An admission controller is a piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object, but after the request is authenticated and authorized.\u0026rdquo; Admission controllers can either validate resources or mutate them, or do both.\nWhy Kyverno Kyverno is not just another admission controller, it can do much more than that. Where an ordinary admission controller can only validate the resources in the cluster. Kyverno not just validates the resources but can also change the resources transparently. when Kyverno receives the API requests, it can validate them, this can also be done in blocking mode.\nBlocking mode means it won\u0026rsquo;t prevent or allow cluster to run but specify that in a policy report.\nOther than this Kyverno also has the ability to mutate the resources transparently. Through Kyverno you can also generate Kubernetes resources. This means you can create new Kubernetes resources of any type or configuration based upon a policy that you define and install in the cluster.\nKyverno Policy A Kyverno policy can be one of:\nCluster scoped Namespace scoped Inside the Kyverno policy we have rules. A single policy has one or multiple rules. Each rule has a match block and can optionally also have an exclude block.\nKyverno\nYou can define scope of the resource you want to apply your policy to, using the following options. You can off-course choose one or multiple of these options:\nResource Kinds Resource Names Labels Annotations Namespaces Namespace Labels (Cluster)Roles Users Groups ServiceAccounts You can also declare what you want from the Policy, do you want to use it for Validation, mutating or something else. Here are the options you can choose in Kyverno:\nValidate the resource Mutate the resource Generate a resource Verify images Example of Kyverno Policy Here is a sample Kyverno Policy, A \u0026ldquo;ClusterPolicy\u0026rdquo; applies to the entire cluster. you can see we have a single rules block inside which we have a match block. We have set validationFailureAction to Audit therefore, it will be allowed even after the resource validation failure what but we should get a report if the resource validation fails.\nThis policy will perform validation of the pods and check the label. If the label is not provided with any value then it will send a message saying \u0026ldquo;The label app.kubernetes.io/name is must\u0026rdquo; .\nScreenshot-2023-09-29-072343\nYou can copy the same policy from the following code:\napiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: require-labels spec: validationFailureAction: Audit background: true rules:\nname: check-for-labels match: any: resources: kinds: -Pod validate: message: \u0026ldquo;The label app.kubernetes.io/name is must\u0026rdquo; pattern: metadata: labels: app.kubernetes.io/name: \u0026ldquo;?*\u0026rdquo; Kyverno Usecases These are all the use cases of Kyverno, Feel free to skip them if you can not understand them as of now:\nSecurity Pod security Workload security Granular RBAC Workload isolation Image signing \u0026amp; verification Workload identity Reports Operations Namespaces-as-a-Service Clusters-as-a-Service Custom CA root certificate injection Automated resource generation Conditional patching of resources Inject sidecar container ConfigMap/Secret copying/syncing Stale resources cleanup Cost Governance Pod requests and limits Namespace quotas Team and app labels QoS management Auto-scalers Advantages of Kyverno over its other alternatives There are a lot of additional features that Kyverno provides compared with other alternative admission controllers, here are a few of them:\nEverything YAML - Simple to write All of the policies in Kyverno are written in YAML, Kyverno takes care of the heavy-lifting while you can simply use YAML for writing the Policy.\nMutating Resources Where an ordinary admission controller can only validate the resources in the cluster. Kyverno not just validates the resources but can also change the resources transparently.\nNamespaces-as-a-Service In Kyverno you can perform tasks like Namespaces-as-a-Service. You can simply create a namespace with a bunch of other resources that are in it. Kyverno can then generate all of those resources for you based on the manifest. You don\u0026rsquo;t have to run bash scripts or spit out resource quotas and limit ranges. You just have to simply write a Kyverno policy in YAML, install it in the cluster and version it. Kyverno will perform all the tasks for you after that.\nImage signing and verification Kyverno can also be used for image signing and verification purposes. Kyverno can do image verification without needing other bolt-on components. The Kyverno policy will need to know which images you want to check. Whenever that image gets built, it will get signed and pushed to the OCI registry.\nSome more features of Kyverno Some more features of Kyverno includes:\nMatching resources using label selectors and wildcards. Validating and mutating resources using overlays. Synchronizing configurations across namespaces. Blocking any non-conformant resources using admission controls, or report policy violations. Kyverno CLI - for testing policies and validating resources in the CI/CD pipeline. managing policies as code using familiar tools like git and kustomize. Conclusion As we discussed in previous sections, Kyverno is not just any admission controller, it is much more than that. An ordinary admission controller can only validate the resources in the cluster. Kyverno not just validates the resources but can also change the resources transparently. We discussed about Kyverno is this article we started off with an introduction to Kyverno, later we dicusses the terms like custom resources and custom controllers. Custom resources are the extensions of the Kubernetes API that are not available in the default Kubernetes installation. While custom controllers are used to encode domain knowledge for specific applications into an extension of Kubernetes API. We then say what Kyverno does? and an example of a Kyverno policy. At the end we discussed the advantages of Kyverno over other admission controllers and various usecases of Kyverno.\nThis marks the end of this article and we hope that this article helped you improve your understanding about Kyverno and policy management in Kubernetes.\nminikube start\nkubectl create -f https://github.com/kyverno/kyverno/releases/download/v1.10.0/install.yaml\nkubectl explain policy.spec.rules.preconditions\nKyverno primarily functions as an admission controller but not just another admission controller. Along with that Kyverno comes with a CLI that you can use outside of Kubernetes\nOnce Kyverno gets installed, it pull down a number of different configurations including there\u0026rsquo;s a bunch of roles it configures so there\u0026rsquo;s fine grain configuration there\u0026rsquo;s a security section in the docs which explains what each one of these roles are doing it also installs some webhooks which is what tells the api server to in you know contact Kyverno based on certain settings and then it\u0026rsquo;s creating some resources custom resources for policies policy reports\nWhy do we need Kyverno, one good example would be this scary piece of code in Kubernetes, if we enter this in the command line\nkubectl run r00t --restart=Never -ti --rm --image lol --overrides \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;hostPID\u0026#34;: true, \u0026#34;containers\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;image\u0026#34;:\u0026#34;public.ecr.aws/h1a5s9h8/alpine:latest\u0026#34;,\u0026#34;command\u0026#34;:[\u0026#34;nsenter\u0026#34;,\u0026#34;--mount=/proc/1/ns/mnt\u0026#34;,\u0026#34;--\u0026#34;,\u0026#34;/bin/bash\u0026#34;],\u0026#34;stdin\u0026#34;:true,\u0026#34;tty\u0026#34;:true,\u0026#34;securityContext\u0026#34;:{\u0026#34;privileged\u0026#34;:true}}]}}\u0026#39; What this is doing is it\u0026rsquo;s running a simple image but then it\u0026rsquo;s doing an nsenter and also elevating privileges for the Pod. So if we run this, the interesting thing that happens if we have no policies or no guardrails in place it that all we need is permissions to run a container in any namespace and we will have access to bash shell. With that we can look at all the containers running.\ncd var/log/containers\nls\nexit\nTutorial `minikube start\nkubectl create -f https://github.com/kyverno/kyverno/releases/download/v1.11.1/install.yaml\n","permalink":"http://localhost:1313/blog/kyverno-intro/","summary":"\u003cp\u003eKyverno is a dynamic admission controller that can be used to govern and enforce how a Kubernetes cluster is used.\u003c/p\u003e\n\u003ch2 id=\"admission-controllers\"\u003eAdmission Controllers\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s first start with admission controllers since Kyverno is an admission controller. In Kubernetes admission controllers are used to intercept requests going to the API servier and one of the two or both the things with them - mutate or/and validate. Kubernetes comes up with built-in admission controllers. The issue with these built-in admission controllers is that they are highly specific and different organizations may have different use cases with admission contollers not being fulfilled by these built-in admission controllers, therefore, for meeting these requirements we have dynamic admission controllers that can be developed as extensions and can be run as webhooks configured at runtime.\u003c/p\u003e","title":"Introduction to Kubernetes Native Policy Management with Kyverno"},{"content":"StatefulSets are API objects in Kubernetes that are used to manage stateful applications. There are two types of applications in Kubernetes, Stateful applications and stateless applications. There are two ways to deploy these applications:\nDeployment (for stateless applications) StatefulSets (for stateful applications) What are Stateful Applications? Those applications that maintain some form of persistent state or data are called stateful applications. The key characteristic that differentiates them from stateless applications is that these applications don\u0026rsquo;t rely on storing data locally and they don\u0026rsquo;t treat each request as independent. They manage data between interactions. Sometimes stateless applications connect to the stateful application to forward the requests to a database.\nWhat are Stateless Applications? Those applications that do not maintain any form of persistent state or data locally are called stateless applications. In stateless applications, each request or interaction is treated independently. These applications are designed to be highly scalable, easy to manage, and fault-tolerant because, unlike Stateful applications, they don\u0026rsquo;t have to track past interactions or requests.\nStateless applications are deployed using deployment component. Deployment is an abstraction of pods and allows you to replicate the application, meaning it allows you to run to 1, 5, 10 or n identical pods of the same stateless application.\nWhat are StatefulSets? In simplest terms StatefulSets are Kubernetes component that is used specifically for stateful applications. These are workload API object used to manage stateful applications. They manage the deployment and scaling of a set of Pods (Creating more replicas or deleting them), and StatefulSets are also responsible for the ordering and uniqueness of these Pods. StatefulSet was released in the Kubernetes 1.9 release.\nStatefulSets will represent the set of pods with different (unique), persistent identities, and elastic hostnames (stable). It makes you assure about the ordering of scaling and deployments. Before understanding StatefulSets, you must understand Kubernetes Deployment.\nHere is an example of a StatefulSet named web:\napiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx\u0026#34; replicas: 4 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: registry.k8s.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] resources: requests: storage: 1Gi When to Use StatefulSets StatefulSets in Kubernetes are ideal for deploying stateful applications that require stable, unique network identifiers, persistent storage, and ordered, graceful deployment and scaling. They are suitable for applications like databases, key-value stores, and messaging queues that require consistent identity and storage.\nExample of Stateful and Stateless Applications Consider a node.js application connected to a MongoDB database. When a request comes to the node.js application it handles the request independently and does not depend on previous data to do that. It handles the request based on the payload in the request itself. This node.js application is an example of Stateless application. Now the request will either update some data in the database or query some data from the database. When node.js forwards that request to MongoDB, MongoDB updates the data based on the previous state of the data or query the data from its storage. For each request it needs to handle data and it depends upon the most up-to-date data or state to be available while node.js is just a pass-through for data updates or queries and it just processes code. Hence the node.js application should be a stateless application while the MongoDB application must be a stateful application.\nSometimes stateless applications connect to the stateful application to forward the requests to a database. This is a good example of a stateless application forwarding request to a stateful application.\nHow to Create a StatefulSet in Kubernetes Here is a step by step tutorial on how to use StatefulSets and some basic operations on StatefulSets.\nCreate a Nginx StatefulSet Application Step 1. Create a StatefulSet file. you can do that by entering the following command:\ntouch example-statefulset.yaml Step 2. Open this file in a code-editor and write the following code into it:\napiVersion: apps/v1 kind: StatefulSet metadata: name: gfg-example-statefulset annotations: description: \u0026#34;This is an example statefulset\u0026#34; spec: selector: matchLabels: app: nginx serviceName: \u0026#34;gfg-example-service\u0026#34; replicas: 3 # remember this, we will have 3 identical pods running template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumes: - name: www persistentVolumeClaim: claimName: myclaim Step 3. Now we have to create a service file and a PersistentVolumeClaim file.\ntouch example-service.yaml touch example-persistentVolumeChain.yaml Create a Service for the StatefulSet Application\nStep 4. Enter the following code into the service file:\napiVersion: v1 kind: Service metadata: name: gfg-example-service annotations: description: \u0026#34;this is an example service\u0026#34; labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx Create a PersistentVolumeClaim for Application Step 5. Enter the following code into the PersistentVolumeClaim file: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: - ReadWriteMany resources: requests: storage: 8Gi # This means we are requesting for 8 GB of storage Now lets apply these changes.\nStep 6. Enter the following command in your terminal to create the gfg-example-statefulset:\nkubectl create -f example-statefulset.yaml This will create our gfg-example-statefulset, you will get a similar result:\nnow if we search our StatefulSets in our terminal by the command\nkubectl get statefulsets we will find our gfg-example-statefulset in the list.\nStep 7. Enter the following command in your terminal to create the gfg-example-service.\nkubectl apply -f example-service.yaml this will create a service with the name \u0026ldquo;gfg-example-service\u0026rdquo;\nStep 8. Let\u0026rsquo;s check our pods and services, for getting the list of pods enter the following command in your terminal:\nkubectl get pods You will get the list of the three gfg- pods that we create though defining three replicas in the example-stateful-set.yaml file. You will get a similar output:\nfor checking the list of services, enter the following command in your terminal:\nkubectl get services This will give you similar output:\nKubectl get services\nSome Operations On StatefulSets Adding a StatefulSet: To add a StatefulSet to your Kubernetes cluster, use the command kubectl create -f [StatefulSet file name], replacing [StatefulSet file name] with the name of your StatefulSet manifest file.\nkubectl create -f StatefulSet_file_name Deleting a StatefulSet: To delete a StatefulSet in Kubernetes, you can use the kubectl delete statefulset [name] command, where [name] is the name of the StatefulSet you want to delete.\nkubectl delete statefulset [name] Editing a StatefulSet: The command kubectl edit statefulset [name] allows you to modify the configuration of a StatefulSet directly from the command line by opening an editor.\nkubectl edit statefulset [name] Scaling of Replicas: The kubectl scale command scales the number of replicas in a StatefulSet named [StatefulSet name] to the specified [Number of replicas].\nkubectl scale statefulset [StatefulSet name] --replicas=[Number of replicas] Step 9. Now let\u0026rsquo;s scale up our pods and check if it works! for scaling up the pods to 6 pods, enter the following command:\nkubectl scale statefulset gfg-example-statefulset --replicas=6 This will create 3 more pods and number of pods are now 6, to get list of pods enter the following command:\nkubectl get pods You will get a similar output:\nStep 10. Now let\u0026rsquo;s scale down pods to 3, for that enter the same command, just change the number of replicas back to 3:\nkubectl scale statefulset gfg-example-statefulset --replicas=3 now if we check the list of pods by\nkubectl get pods you will see only 3 pods running:\nIn this way we can create StatefulSets, scale them up and then scale them down as well. Make sure to delete the StatefulSet and the service before closing the terminal.To know more commands of of kubectl refer to Kubectl Command Cheat Sheet.\nHow Stateful Applications Work? In stateful applications like MySQL, multiple pods cannot simultaneously read and write data to avoid data inconsistency. One pod is designated as the master pod, responsible for writing and changing data, while others are designated as slave pods, only allowed to read data. Each pod has its own replica of the data storage, ensuring data isolation and independence. Synchronization mechanisms are employed to ensure that all pods have the same data state, with slave pods updating their data storage when the master pod changes data. Continuous synchronization is necessary to maintain data consistency among all pods in the stateful applications.\nExample: Let\u0026rsquo;s say we have one master and two slave pods of MySQL. Now what happens when a new pod replica joins the existing setup? because now that new pod also needs to create its own storage and take care of synchronizing it what happens is that it first clones the data from the previous pod and then it starts continuous synchronization to listen for any updates by master pod. since each pod has its own data storage (persistent volume) that is backed up by its own physical storage which includes the synchronized data and the state of the pod. Each pod has its own state which has information about whether it\u0026rsquo;s a master pod or a slave pod and other individual characteristics. All of this gets stored in the pods own storage. Therefore when a pod dies and gets replaced the persistent pod. Identifiers make sure that the storage volume gets reattached to the replacement pod. In this way even if the cluster crashes, it is made sure that data is not lost.\nConclusion In this article we discussed about how to use Kubernetes StatefulSets. StatefulSets are Kubenetes components used to deploy stateful applications. Stateful applications are those applications that maintain some form of persistent state or data. A good example would be any application with a database. We discussed about how to deploy an stateful application using StatefulSets. After that we discussed how Stateful applications work? At the end we discussed the difference between StatefulSet and deployment which basically moves around the point that deployment are used to deploy stateless application and StatefulSets are used to deploy statefull applications. We will end this article by addressing some FAQs.\nKubernetes StatefulSets - FAQs How do I increase volume size in Kubernetes? To increase volume size in Kubernetes, you need to modify the PersistentVolumeClaim (PVC) specification by changing the storage size. Then, Kubernetes automatically provisions additional storage to meet the new size requirement, provided the underlying storage class supports dynamic provisioning.\nWhen to use hostPath in Kubernetes? The hostPath volume type in Kubernetes is suitable for scenarios where you need to access files or directories on the node\u0026rsquo;s filesystem directly within a pod. It\u0026rsquo;s commonly used for accessing node-specific resources or for sharing data between containers on the same node.\nWhat is the difference between PVC StatefulSet and deployment? PersistentVolumeClaims (PVCs) in StatefulSets are used to provide stable, persistent storage for individual pods, ensuring data persistence and identity across pod restarts. In contrast, deployments typically utilize ephemeral volumes, suitable for stateless applications where data persistence is not a requirement.\nCan We Deploy A Stateless Application Using StatefulSets? While technically possible, it\u0026rsquo;s not recommended to deploy stateless applications using StatefulSets. StatefulSets are specifically designed for stateful applications requiring stable, unique identifiers and persistent storage. Deploying stateless applications with StatefulSets can introduce unnecessary complexity and resource overhead.\nIs stateless better than stateful? It depends on the specific requirements of the application. Stateless applications are simpler to manage and scale horizontally, while stateful applications maintain data integrity and are better suited for certain workloads like databases or messaging systems.\n","permalink":"http://localhost:1313/blog/statefulsets/","summary":"\u003cp\u003eStatefulSets are API objects in Kubernetes that are used to manage stateful applications. There are two types of applications in Kubernetes, Stateful applications and stateless applications. There are two ways to deploy these applications:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeployment (for stateless applications)\u003c/li\u003e\n\u003cli\u003eStatefulSets (for stateful applications)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"what-are-stateful-applications\"\u003eWhat are Stateful Applications?\u003c/h2\u003e\n\u003cp\u003eThose applications that maintain some form of persistent state or data are called stateful applications. The key characteristic that differentiates them from stateless applications is that these applications don\u0026rsquo;t rely on storing data locally and they don\u0026rsquo;t treat each request as independent. They manage data between interactions. Sometimes stateless applications connect to the stateful application to forward the requests to a database.\u003c/p\u003e","title":"Kubernetes Statefulsets"},{"content":"Unit testing using Golang Let\u0026rsquo;s first start with what unit testing is. Unit Testing is a way of testing where the behavior of a unit of software is tested to determine if it works properly and exhibits the expected behavior. But why do we need unit testing? There are multiple reasons for that. See, how can we ensure that even if we change code, the code still works and there are no new bugs introduced, it\u0026rsquo;s better to detect them early than to wait and run the entire application in order to check a small code. In this way, unit tests help ensure quality code, help in catching bugs early, and provide a safety net if the code changes in the future.\nWhat is unit testing? As per IEEE (Institute for Electrical and Electronics Engineers), unit testing is the “testing of individual hardware or software units or groups of related units”. Unit testing is a software testing technique where individual components or units of a software application are tested in isolation.\nBy unit, we mean the smallest testable part of a code. mostly, it is a single function in Golang. The goal of unit testing is to ensure that each of these units functions correctly (produces expected results).\nThe term \u0026ldquo;unit\u0026rdquo; in unit testing refers to the smallest testable part of a software program, typically a single function, method, or class. The primary goal of unit testing is to ensure that each of these individual units of code functions correctly and produces the expected results.\nWhy Unit tests in Go? • Helps in verifying small changes (refactoring, debugging) quickly • Measures the quality of the code\n• Helps in understanding the complex logics\n• Helps in understanding the cause of failure quickly\n• Helps the reviewer to understand the fixes/changes\n• Great way to learn about a language as well as the project.\nLet\u0026rsquo;s create a basic unit test: Step 1. Start with initializing a go package\ngo mod init github.com/1shubham7/basic-unit-test\nStep 2. Download a unit testing package called go-money\ngo get github.com/Rhymond/go-money\nStep 3. Create this file structure:\nCreate a internal folder, inside that create a order folder, and inside the order folder, create a order.go file.\nStep 4. Start coding in the order.go file\npackage order\rimport (\r\u0026#34;github.com/Rhymond/go-money\u0026#34;\r\u0026#34;fmt\u0026#34;\r)\rtype Order struct {\rID string\rCurrencyAlphaCode string\rItems []Item\r}\rtype Item struct {\rID string\rQuantity uint\rUnitPrice *money.Money\r}\rfunc (o Order) ComputeTotal() (*money.Money, error) {\ramount := money.New(0, o.CurrencyAlphaCode)\rfor _, item := range o.Items {\rvar err error\ramount, err = amount.Add(item.UnitPrice)\rif err!= nil {\rreturn nil, fmt.Errorf(\u0026#34;not adding item elements, error: %w\u0026#34;, err)\r}\r}\rreturn amount, nil\r} Step 5. Enter the following command to download testify\n/go get github.com/stretchr/testify\nStep 6. Create a order_test.go file and enter the following code inside it:\npackage order\rimport (\r\u0026#34;testing\u0026#34;\r\u0026#34;github.com/Rhymond/go-money\u0026#34;\r\u0026#34;github.com/stretchr/testify/assert\u0026#34;\r)\rfunc TestOrder(t *testing.T) {\ro := Order{\rID: \u0026#34;100\u0026#34;,\rCurrencyAlphaCode: \u0026#34;INR\u0026#34;,\rItems: []Item{\r{\rID: \u0026#34;500\u0026#34;,\rQuantity: 2,\rUnitPrice: money.New(100, \u0026#34;INR\u0026#34;),\r},\r},\r}\rtotal, err := o.ComputeTotal()\rassert.NoError(t, err)\rassert.Equal(t, 200, total.Amount())\rassert.Equal(t, \u0026#34;INR\u0026#34;, total.Currency())\r} Step 7. Now this command is used to run all the unit test in that perticular directory, enter the following command:\ngo test ./...\nit will give you the result:\n--- FAIL: TestOrder (0.00s) order_test.go:25: Error Trace: D:/Files/Golang/Basic Unit Test/internal/order/order_test.go:25\rError: Not equal: expected: int(200) actual : int64(100) Test: TestOrder order_test.go:26: Error Trace: D:/Files/Golang/Basic Unit Test/internal/order/order_test.go:26\rError: Not equal: expected: string(\u0026#34;INR\u0026#34;) actual : *money.Currency(\u0026amp;money.Currency{Code:\u0026#34;INR\u0026#34;, NumericCode:\u0026#34;356\u0026#34;, Fraction:2, Grapheme:\u0026#34;₹\u0026#34;, Template:\u0026#34;$1\u0026#34;, Decimal:\u0026#34;.\u0026#34;, Thousand:\u0026#34;,\u0026#34;}) Test: TestOrder FAIL FAIL github.com/1shubham7/basic-unit-test/internal/order 1.001s FAIL Step 8. to fix this up, we have to make some changes in the code:\nMake these changes to the code:\norder.go:\npackage order\nimport (\r\u0026#34;github.com/Rhymond/go-money\u0026#34;\r\u0026#34;fmt\u0026#34;\r)\rtype Order struct {\rID string\rCurrencyAlphaCode string\rItems []Item\r}\rtype Item struct {\rID string\rQuantity uint\rUnitPrice *money.Money\r}\rfunc (o Order) ComputeTotal() (*money.Money, error) {\ramount := money.New(0, o.CurrencyAlphaCode)\rfor _, item := range o.Items {\rvar err error\ramount, err = amount.Add(item.UnitPrice.Multiply(int64(item.Quantity)))\rif err!= nil {\rreturn nil, fmt.Errorf(\u0026#34;not adding item elements, error: %w\u0026#34;, err)\r}\r}\rreturn amount, nil\r} order_test.go:\npackage order\rimport (\r\u0026#34;testing\u0026#34;\r\u0026#34;github.com/Rhymond/go-money\u0026#34;\r\u0026#34;github.com/stretchr/testify/assert\u0026#34;\r)\rfunc TestOrder(t *testing.T) {\ro := Order{\rID: \u0026#34;100\u0026#34;,\rCurrencyAlphaCode: \u0026#34;INR\u0026#34;,\rItems: []Item{\r{\rID: \u0026#34;500\u0026#34;,\rQuantity: 2,\rUnitPrice: money.New(100, \u0026#34;INR\u0026#34;),\r},\r},\r}\rtotal, err := o.ComputeTotal()\rassert.NoError(t, err)\rassert.Equal(t, int64(200), total.Amount())\rassert.Equal(t, \u0026#34;INR\u0026#34;, total.Currency().Code)\r} Now if you enter the same command:\ngo test ./...\nIt works perfectly fine: And that\u0026rsquo;s how we create a unit test. See you in some next tutorial. Thanks for reading.\n","permalink":"http://localhost:1313/blog/testing/","summary":"\u003ch1 id=\"unit-testing-using-golang\"\u003eUnit testing using Golang\u003c/h1\u003e\n\u003cp\u003eLet\u0026rsquo;s first start with what unit testing is. Unit Testing is a way of testing where the behavior of a unit of software is tested to determine if it works properly and exhibits the expected behavior. But why do we need unit testing? There are multiple reasons for that. See, how can we ensure that even if we change code, the code still works and there are no new bugs introduced, it\u0026rsquo;s better to detect them early than to wait and run the entire application in order to check a small code. In this way, unit tests help ensure quality code, help in catching bugs early, and provide a safety net if the code changes in the future.\u003c/p\u003e","title":"Unit testing using Golang - a beginner's guide"},{"content":"Let\u0026rsquo;s first start with a quick intro about what GSoC is :\nSo, GSoC (Google Summer of Code) is an annual program launched by Google that offers university students from around the world the opportunity to work on open source software projects during the summer break. The program aims to bring together talented students, open source organizations, and mentors to contribute to open source software development and expand access to key technology initiatives.\nThere were about 43,000 applicants and about 7,700 proposals were submitted. Out of which 960 proposals were accepted. So, every 1 in 8 proposals was submitted, making GSoC at least lesser competitive than the IIT-JEE exam.\nMy Proposal I got to know about GSoC very early but due to my college exams and also me focusing more on my skills rather than open source I messed up this great opportunity. my semester exams ended on 1 April 2023 and the last date for proposal submission was 4th April 2023. I completed my proposal just 15 min before the deadline.\nHere is the link to my LinkedIn and Twitter where I have uploaded my proposal since I could not find any options in Hashnode to upload a PDF.\nLinkedIn : GSOC/1shubham7\nMistakes I made Mistake 1. Thinking I have to be perfect in order to contribute to open source. I thought that I must at least know 90% of the technology in order to contribute to such complex code bases while the truth is open source has something for everybody. You can start your contribution journey by solving the \u0026ldquo;Good First Issues\u0026rdquo;. Here is a link to a great website that finds you good first issues for the technology you are decent at:\ngoodfirstissue.dev\nJust go and try contributing, learn how to set up the project locally and contribute with whatever little knowledge you have, Of course, it\u0026rsquo;s even better you if have good knowledge of the technology.\nMistake 2. To select the wrong project. Go to the GSoC website and have a look at the organizations coming there, there are some organizations that are not able to complete their projects. Make sure to see the project history of the organizations. Make sure that the organization is consistently participating in GSoC every year and getting the projects done. For me, I choose a good organization but the project I was trying to contribute to was not very active, there was very less activity in the GitHub repo of that project.\nMistake 3. Choose a project with active mentors Learn from the mistakes I made, do check out the GitHub repo of the project before selecting it, and make sure that the project members and the mentors are active and helping newcomers with their queries. It is very rare that an inactive project gets completed.\nMistake 4. Start contributing early The earlier you start, the better it is. Make sure you start contributing to the project specifically at least 3-4 months and start contributing to any projects from now so to have a stronger GitHub profile.\nMistake 5. (You can ignore this one depending on your situation) I did not study much for college exams but I have now realized that I could have even studied lesser, I was easily fooled by my teachers since it was my first semester, now that I am in my second sem, I study lesser and focus more on open-source. You can ignore this advice depending on your situation. Since I come from a tier-3 college, I know we don\u0026rsquo;t have good placements so I am not even considering college placements, I am prepping for off-campus placements. If you are from a decent college, do focus a little bit more on your college exams.\nConclusion : In conclusion, as someone who missed out on GSoC 2023 due to mistakes made during the application process, I hope that my experience and advice can help future applicants avoid similar mistakes. I learned that open source has something for everyone, and that contributing to projects with active mentors and established project histories can increase one\u0026rsquo;s chances of being accepted into GSoC. I also realized that starting early and consistently contributing to open source projects can help build a strong GitHub profile, which can benefit not only GSoC applications but also future career opportunities. I encourage all students to consider applying for GSoC and not be discouraged by any initial challenges. The program provides a valuable learning opportunity and a chance to contribute to the open source community.\nThese were the mistakes I made. Hope you learn from my mistakes and have a beautiful GSoC journey. Hope this article will help you, if you are reading this and in the future you get selected, do tag me on Twitter: 1Shubham7. I will also be applying for GSoC2024, in the second year of my college.\nI wish you all the luck with your GSoC. Thank you for reading.\nMy Socials Twitter: 1Shubham7\nGitHub: 1Shubham7\nLinkedIn: Shubham Singh\n","permalink":"http://localhost:1313/blog/gsoc/","summary":"\u003cp\u003eLet\u0026rsquo;s first start with a quick intro about what GSoC is :\u003c/p\u003e\n\u003cp\u003eSo, GSoC (Google Summer of Code) is an annual program launched by Google that offers university students from around the world the opportunity to work on open source software projects during the summer break. The program aims to bring together talented students, open source organizations, and mentors to contribute to open source software development and expand access to key technology initiatives.\u003c/p\u003e","title":"5 Mistakes to Avoid for cracking GSoC Proposal: Lessons from My GSoC'23 Experience"},{"content":"Databases: just a digital tool that stores and organizes information in a way that makes it easy to access and merge. Therefore now we can write a technical definition of Databases that is\n\u0026ldquo;In technical language, a database is a structured collection of data that is stored electronically on a computer system. It is designed to efficiently manage, store, retrieve, and update large amounts of data.\u0026rdquo;\nso where are databases needed?\nanywhere where you can think you need to store data in a proper manner. From your class\u0026rsquo;s students list to Facebook\u0026rsquo;s user list.\ntypes of databases :\nRelational Databases - These databases store data in tables with rows and columns, and the relationships between tables are defined by keys.\nNoSQL Databases - These databases are designed to handle unstructured data and can store data in a variety of formats, including key-value pairs, documents, and graphs.\nObject-oriented Databases - These databases store data as objects, which can include data and code. They are often used in software development. (You must have heard of Object-oriented Programming in Java, c++, etc)\nGraph Databases - These databases store data in nodes and edges and are particularly useful for storing data with complex relationships.\nHierarchical Databases - These databases organize data in a tree-like structure, with each node having one parent and multiple children.\nNetwork Databases - These databases store data in a more flexible structure than hierarchical databases, allowing nodes to have multiple parents and children.\nDistributed Databases - These databases are spread across multiple computers or servers, allowing for increased scalability and fault tolerance.\nAnd that\u0026rsquo;s it you are done with databases. Make sure to check out our blog on DBMS, the continuation of this topic.\n","permalink":"http://localhost:1313/blog/database/","summary":"\u003cp\u003eDatabases: just a digital tool that stores and organizes information in a way that makes it easy to access and merge. Therefore now we can write a technical definition of Databases that is\u003c/p\u003e\n\u003cp\u003e\u0026ldquo;In technical language, a database is a structured collection of data that is stored electronically on a computer system. It is designed to efficiently manage, store, retrieve, and update large amounts of data.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eso where are databases needed?\u003c/p\u003e","title":"Databases - the easy way"},{"content":"Python includes 6 types of built-in datatypes -\nNumeric datatype\nBoolean datatype\nSequence datatype\nSets\nDictionaries\nNone\nNumeric datatype this datatype includes integer, float, and complex numbers. Unlike other languages, python allows you to reassign float value to a variable with integer or vice versa.\na = 10\ra = 10.5\rprint(a) #this will output 10.5 Boolean datatype This is a special data type that represents one of two possible values: True or False. Boolean values are often used in conditional statements and loops. Remember the first letter in \u0026ldquo;True\u0026rdquo; and \u0026ldquo;False\u0026rdquo; must be capitalized.\na = True\rif (a==True):\rprint(\u0026#34;a is true\u0026#34;)\relse :\rprint(\u0026#34;a is not true\u0026#34;) Sequence These include strings, lists, tuples, and byte sequences. Strings are used to represent textual data, while lists and tuples are used to store collections of items. Byte sequences are similar to strings but represent binary data.\nmy_tuple = (\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;,\u0026#34;d\u0026#34;,5,6,7,8)\rprint(my_tuple) Sets These are unordered collections of unique items. Python also has a frozenset type that is similar to sets, but is immutable.\nmy_set = {1,2,3,4}\rprint(my_set) Dictionaries These are unordered collections of key-value pairs. They are used to store and retrieve data based on a unique value.\nmy_dict = {\u0026#34;name\u0026#34;:\u0026#34;Shubham\u0026#34; , \u0026#34;age\u0026#34;:19}\rprint(my_dict[\u0026#39;name\u0026#39;])\rprint(my_dict) None This is a special data type that represents the absence of a value. In other languages like Java, C++, c, Javascript, etc null is used rather than None.\na = null1\nPlease Follow my coming blogs on related topics.\nThank you for your time.\n","permalink":"http://localhost:1313/blog/python_datatypes/","summary":"\u003cp\u003ePython includes 6 types of built-in datatypes -\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eNumeric datatype\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBoolean datatype\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSequence datatype\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSets\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDictionaries\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNone\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"numeric-datatype\"\u003eNumeric datatype\u003c/h2\u003e\n\u003cp\u003ethis datatype includes integer, float, and complex numbers. Unlike other languages, python allows you to reassign float value to a variable with integer or vice versa.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ea = 10\r\n\r\na = 10.5\r\n\r\nprint(a) #this will output 10.5\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"boolean-datatype\"\u003eBoolean datatype\u003c/h2\u003e\n\u003cp\u003eThis is a special data type that represents one of two possible values: True or False. Boolean values are often used in conditional statements and loops. Remember the first letter in \u0026ldquo;True\u0026rdquo; and \u0026ldquo;False\u0026rdquo; must be capitalized.\u003c/p\u003e","title":"Datatypes in Python"},{"content":"What is DBMS? if you have created or are going to create a database, obviously, you will have to add, remove, and edit the data in it (i.e. manage data in it) - DBMS can be used for that.\nIn technical language, \u0026ldquo;a DBMS (Database Management System) is a software system that allows users to store, retrieve, modify, and manage data in a database. It provides a structured way to organize and manage large amounts of data efficiently and securely.\u0026rdquo;\nClassification : there are mainly 4 types of DBMS -\nRelational Database Management System (RDBMS) It is the most commonly used DBMS type. In this type of DBMS, data is organized into tables, and relationships between tables are established using keys. You must have heard about this kind of DBMS. Some examples are MySQL, Oracle, PostgreSQL\nObject-Oriented Database Management System (OODBMS) In this type of DBMS, data is stored in the form of objects. OODBMS is mostly used for developing object-oriented applications. Examples of OODBMS are Zope and Versant.\nHierarchical Database Management System (HDBMS) In this type of DBMS, data is organized in a tree-like hierarchical structure. HDBMS is very useful for handling a large amount of unchanging data with a defined hierarchy. Examples of HDBMS include IBM\u0026rsquo;s Information Management System (IMS).\nNetwork Database Management System In this type of DBMS, data is organized in a more flexible network model. Network DBMS is used to store complex data structures that contain many-to-many relationships by allowing each record to have multiple parent and child records. Examples of Network DBMS include Integrated Data Store (IDS) and Integrated Database Management System (IDMS).\nConclusion In conclusion, there are four main types of Database Management Systems (DBMS) which include the Relational Database Management System (RDBMS), Object-Oriented Database Management System (OODBMS), Hierarchical Database Management System (HDBMS), and Network Database Management System. Each of these DBMS types has its unique way of organizing data, and they can be used for different purposes depending on the requirements of an application. So, understanding the different types of DBMS can help developers choose the right type of data management system for their applications.\n","permalink":"http://localhost:1313/blog/dbms/","summary":"\u003ch2 id=\"what-is-dbms\"\u003eWhat is DBMS?\u003c/h2\u003e\n\u003cp\u003eif you have created or are going to create a database, obviously, you will have to add, remove, and edit the data in it (i.e. manage data in it) - DBMS can be used for that.\u003c/p\u003e\n\u003cp\u003eIn technical language, \u003cem\u003e\u003cstrong\u003e\u0026ldquo;a DBMS (Database Management System) is a software system that allows users to store, retrieve, modify, and manage data in a database. It provides a structured way to organize and manage large amounts of data efficiently and securely.\u0026rdquo;\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e","title":"DBMS - the easy way"},{"content":"You must have heard of SCSS or SASS, but do you know the full form of the two tools, how similar these two terms are ? and what are the differences between them? join me in my exploration of these two CSS Pre-processors.\nWhat is SASS ? SASS (Syntactically Awesome Style Sheets) is a CSS pre-processor that allows you to write more maintainable and flexible code for your stylesheets.\nSass works by providing a set of extensions to CSS, which are then compiled to standard CSS syntax, so they can be used in your web project. Here is an example of how a file with SASS looks like.\n$primary-color: #ff0000;\r$secondary-color: #00ff00;\r@mixin center-element {\rdisplay: flex;\rjustify-content: center;\ralign-items: center;\r}\r.header {\rbackground-color: $primary-color;\rcolor: #ffffff;\rpadding: 10px;\rh1 {\rfont-size: 24px;\rmargin-bottom: 10px;\r}\r\u0026amp;.large {\rfont-size: 32px; }\r\u0026amp;.small {\rfont-size: 18px;\r}\r}\r.container {\r@include center-element;\rbackground-color: $secondary-color;\rheight: 200px; width: 200px;\r} What is SCSS ? SCSS stands for Sassy CSS. It is also a CSS preprocessor. SCSS is a superset of CSS, which means that any valid CSS code is also valid SCSS code. SCSS syntax is similar to CSS syntax with some additional features and enhancements, such as variables, nesting, mixins, inheritance, and more.\nSCSS can help to shorten your CSS code, make it more maintainable, and easier to read. For example, you can define variables in SCSS, which allows you to use them throughout your CSS code.\nExample of SCSS code: $primary-color: #007bff;\r$secondary-color: #6c757d;\r@mixin center-element {\rdisplay: flex;\ralign-items: center;\rjustify-content: center;\r}\r.container {\rwidth: 100%;\r.header {\rbackground-color: $primary-color;\rcolor: white;\rpadding: 20px;\rh1 {\rfont-size: 24px;\r}\r}\r.main {\rbackground-color: white;\rpadding: 20px;\r.content {\rmargin-bottom: 20px;\rp {\rcolor: $secondary-color;\r}\r}\r}\r.footer {\rbackground-color: $secondary-color;\rcolor: white; padding: 20px;\r}\r}\r.centered-box {\r@include center-element;\rwidth: 200px;\rheight: 200px;\rbackground-color: $primary-color;\rcolor: white;\r} Difference between SCSS and SASS: SCSS SCSS is a superset of CSS, which means that any valid CSS code is also valid SCSS code.\nSCSS uses the same syntax as CSS, with curly braces {} and semicolons ;. It supports all the features of CSS, including nested selectors, variables, mixins, and more. SCSS files have the .scss file extension. SASS Sass has its own syntax, which is more concise and indentation-based.\nIt doesn\u0026rsquo;t use curly braces {} or semicolons ;. Instead, indentation is used to indicate nesting and separate properties. It doesn\u0026rsquo;t require the use of brackets or semicolons, which can make the code look cleaner and more readable. Sass supports all the features of SCSS, including nested selectors, variables, mixins, and more. Sass files have the .sass file extension. Conclusion In conclusion, SCSS and SASS are both CSS pre-processors that allow developers to write more maintainable, flexible, and organized code by extending the capabilities of standard CSS syntax. SCSS is a superset of CSS and uses the same syntax as CSS, while SASS has its own syntax that is more concise and uses indentation instead of curly braces and semicolons. Both of these preprocessors provide features such as variables, nesting, mixins, inheritance, and more, which can help to make code more readable and maintainable. It\u0026rsquo;s up to personal preference which pre-processor to use, and both have their pros and cons.\nThank you for reading.\nFollow me for more such blogs.\n","permalink":"http://localhost:1313/blog/sass/","summary":"\u003cp\u003eYou must have heard of SCSS or SASS, but do you know the full form of the two tools, how similar these two terms are ? and what are the differences between them? join me in my exploration of these two CSS Pre-processors.\u003c/p\u003e\n\u003ch2 id=\"what-is-sass-\"\u003eWhat is SASS ?\u003c/h2\u003e\n\u003cp\u003eSASS (Syntactically Awesome Style Sheets) is a CSS pre-processor that allows you to write more maintainable and flexible code for your stylesheets.\u003c/p\u003e\n\u003cp\u003eSass works by providing a set of extensions to CSS, which are then compiled to standard CSS syntax, so they can be used in your web project. Here is an example of how a file with SASS looks like.\u003c/p\u003e","title":"Difference between SASS and SCSS"},{"content":"What are artifacts? In the broadest sense, artifacts are objects or items created or modified by humans that have some cultural significance. Similarly, In the context of DevOps, artifacts are files or sets of files that are produced by the build process of a software development project.\nYou must have heard of JAR files, ZIP files or this one for sure - Docker Images. Yes, These are also Artifacts.\nThese files are usually generated by a build tool (such as Maven, Gradle, or Ant for Java projects), which takes the source code and turns it into an executable program that can be run on a target environment.\nArtifact Repository : An artifact repository is a tool used in software development that stores and manages artifacts produced during the software build process. It serves as a central repository for storing, sharing and distributing.\nNote : every artifact format (ZIP, JAR, etc) needs a different artifact repository.\nNow a project being built by large start-ups like RedHat, Civo, Zulip, or your next company will surely have files or artifacts written in multiple languages. Some in Java, some in Golang and some in Javascript. Therefore they will require different software for every Artifact repository will be really chaotic and complex for the Engineers. To solve this problem we have Artifact Repository Managers.\nArtifact Repository Manager : An artifact repository manager is a tool used to manage and store artifacts produced during the software development lifecycle. It is a type of artifact repository but with additional features and functionalities aimed at managing the artifact repository effectively.\nNexus is a very popular artifact repository manager. Artifactory and Apache Archiva are two more popular artifact repository managers.\nConclusion: In conclusion, artifacts are files or sets of files produced during the software build process, which are stored and managed by artifact repository managers. This helps in effective management, sharing, and distribution of artifacts across the software development lifecycle. Popular artifact repository managers include Nexus, Artifactory, and Apache Archiva, each with their own unique features and capabilities aimed at managing artifact repositories effectively.\nThank you for reading.\n","permalink":"http://localhost:1313/blog/artifacts/","summary":"\u003ch2 id=\"what-are-artifacts\"\u003eWhat are artifacts?\u003c/h2\u003e\n\u003cp\u003eIn the broadest sense, artifacts are objects or items created or modified by humans that have some cultural significance. Similarly, In the context of DevOps, artifacts are files or sets of files that are produced by the build process of a software development project.\u003c/p\u003e\n\u003cp\u003eYou must have heard of JAR files, ZIP files or this one for sure - Docker Images. Yes, These are also Artifacts.\u003c/p\u003e\n\u003cp\u003eThese files are usually generated by a build tool (such as Maven, Gradle, or Ant for Java projects), which takes the source code and turns it into an executable program that can be run on a target environment.\u003c/p\u003e","title":"Everything about Artifacts for DevOps"},{"content":"Kubernetes Deployment Strategies Kubernetes or K8s, is an open-source container orchestration technology used to automate the manual processes of deploying, managing, and scaling applications with the help of containers. Originally developed by engineers at Google, Kubernetes was donated to the CNCF (Cloud Native Computing Foundation) in 2015. One of Kubernetes\u0026rsquo; key strengths is its ability to handle application deployments seamlessly while ensuring minimal application downtime. This article discusses different Kubernetes deployment strategies for deploying new versions of your application.\nKubernetes Deployment Strategies 1. Blue/Green Deployment The blue/green deployment strategy is a powerful technique where developers use Kubernetes\u0026rsquo;s ability to run multiple identical environments concurrently. In this approach, you maintain two separate production environments: the \u0026ldquo;blue\u0026rdquo; environment and the \u0026ldquo;green\u0026rdquo; environment. At any given time, one environment (e.g., blue) serves the live traffic, while the other (e.g., green) is used to make changes for future updates.\nWhen deploying a new version of your application, you create and configure the new green environment with the updated code and resources. Once the green environment is fully operational and validated, you switch the traffic routing from the blue environment to the green environment. This can be done seamlessly, and the traffic effectively cuts over to the new version. The blue/green deployment strategy ensures zero downtime during deployments and provides a straightforward rollback mechanism if issues arise with the new version. Simply reroute traffic back to the blue environment, and then investigate and address any problems with the green environment without impacting live users.\n2. Canary Deployment The canary deployment strategy is a more gradual and controlled approach compared to the blue/green deployment strategy. Instead of switching all traffic to the new version at once, you introduce the new version to a subset of your users. This subset acts as a \u0026ldquo;canary\u0026rdquo; so you can monitor the new version\u0026rsquo;s performance and behavior closely before rolling it out to all users.\nIn the canary deployment strategy, start by routing a small percentage of traffic (e.g., 5-10%) to the new version, and gradually increase the traffic share as you gain more confidence in the new version\u0026rsquo;s stability and functionality. This strategy is particularly useful when deploying significant application changes or introducing new features. The canary deployment strategy enables you to catch and mitigate potential issues affecting only part of your users, so they don\u0026rsquo;t impact your entire user base. If any problems are detected during the canary deployment, you can quickly roll back the new version and minimize the impact on your users.\n3. Rolling Update The rolling update strategy is the default deployment strategy in Kubernetes and is popular among small startups and companies. In the rolling update strategy, you gradually replace the old Pods with new ones, ensuring that a certain number of Pods (specified by the \u0026lsquo;maxUnavailable\u0026rsquo; and \u0026lsquo;maxSurge\u0026rsquo; parameters) are available during the update process. This strategy minimizes application downtime and ensures a smooth transition between new and old versions.\nHow the rolling update process works: Kubernetes creates a new ReplicaSet with the desired number of replicas for the new version of the application. It gradually scales down the old ReplicaSet and scales up the new one, maintaining the desired number of available Pods. This process continues until all old Pods are replaced with new ones. 4. Recreate The recreate strategy is a straightforward approach where Kubernetes will terminate all existing Pods before creating new ones with the updated configuration (new version). Use this strategy for applications where you are ready to see some downtime during deployments or for applications that require a complete restart to apply configuration changes.\nHow the Recreate strategy works: Kubernetes scales down the old ReplicaSet to zero replicas (terminating all existing Pods). Once all old Pods are terminated, Kubernetes creates a new ReplicaSet with the desired number of replicas for the new version of the application. Note: The recreate strategy is simple and efficient but will lead to downtime during the update process, which may not be acceptable for many types of applications.\n5. A/B Testing The A/B testing deployment strategy involves testing new features or configurations with a small percentage of users. Run two versions of your application simultaneously, routing a portion of the traffic to each version based on predefined rules or conditions.\nThis strategy allows you to gather user feedback on the new version\u0026rsquo;s performance or behavior and make changes accordingly before rolling it out to all users. Based on the data from A/B testing, decide whether to proceed with the new version, make changes, or roll back to the previous version.\n6. Ramped Deployment The ramped deployment strategy is similar to the rolling update strategy with a small variation. Instead of replacing Pods one by one, it introduces a new ReplicaSet with the updated version of your application and gradually increases the number of replicas until the desired state is reached.\nHow the Ramped Deployment strategy works: Start by creating a new ReplicaSet with a small number of replicas (e.g., 1 or 2) for the new version. As the new Pods become ready, Kubernetes will gradually scale up the new ReplicaSet and scale down the old ReplicaSet until the old ones become zero and new ones take their place. 7. Traffic Splitting The traffic splitting deployment strategy involves routing incoming traffic to different versions of your application based on predetermined rules or conditions. Often used with other deployment strategies (e.g., blue/green or canary deployments), traffic splitting helps control the distribution of traffic between different versions of your application.\nTraffic splitting can be achieved using Kubernetes Services and Ingress resources. Configure them to route traffic based on factors such as:\nHTTP headers Cookies Source IP addresses Custom rules Conclusion Kubernetes is a powerful container orchestration platform that simplifies the deployment and management of containerized applications at scale. One of its key strengths is handling rolling updates and deployments seamlessly. In this article, we explored various Kubernetes deployment strategies, including blue/green, canary, rolling update, recreate, A/B testing, ramped deployment, and traffic splitting. Each strategy caters to different requirements and scenarios, providing you with the flexibility to choose the approach that best suits your application\u0026rsquo;s needs. By following the best practices outlined in this article, you can ensure a smooth and successful deployment process while minimizing downtime and potential issues. Make sure to follow other articles on GeeksforGeeks to learn more about Kubernetes and its advanced features.\nFAQs about Kubernetes Deployment Strategies Q. What is a rolling update deployment?\nA rolling update deployment is a deployment technique in Kubernetes where we gradually replace the old Pods with new ones, ensuring that a certain number of Pods (specified by the \u0026lsquo;maxUnavailable\u0026rsquo; and \u0026lsquo;maxSurge\u0026rsquo; parameters) are available during the update process. Q. What happens in blue/green deployment?\nIn the blue/green deployment strategy, you maintain two separate production environments: the \u0026ldquo;blue\u0026rdquo; environment and the \u0026ldquo;green\u0026rdquo; environment. At any given time, one environment (e.g., blue) serves the live traffic, while the other (e.g., green) is used to make changes for future updates. Q. What is the difference between blue/green and canary deployments?\nIn blue/green deployment, we switch all traffic from one environment to the other environment altogether. In canary deployment, we introduce the new version to only a part of our users or traffic and gradually increase this percentage. Q. How can I implement A/B testing in Kubernetes?\nTo implement A/B testing, run two versions of your application together. Route a certain percentage of your users to each version using Kubernetes Services and Ingress resources. Q. Why should one use the traffic splitting deployment strategy?\nThe traffic splitting deployment strategy can be used with other deployment strategies, such as blue/green or canary deployments, to control the distribution of traffic between different versions of your application. ","permalink":"http://localhost:1313/blog/k8s-depl-stratigies/","summary":"\u003ch1 id=\"kubernetes-deployment-strategies\"\u003eKubernetes Deployment Strategies\u003c/h1\u003e\n\u003cp\u003eKubernetes or K8s, is an open-source container orchestration technology used to automate the manual processes of deploying, managing, and scaling applications with the help of containers. Originally developed by engineers at Google, Kubernetes was donated to the CNCF (Cloud Native Computing Foundation) in 2015. One of Kubernetes\u0026rsquo; key strengths is its ability to handle application deployments seamlessly while ensuring minimal application downtime. This article discusses different Kubernetes deployment strategies for deploying new versions of your application.\u003c/p\u003e","title":"Kubernetes Deployment Strategies"},{"content":"Contributing to open-source projects as a beginner can be a little overwhelming. We come across many terms which might sound alien to us. I remember myself 3 months ago, trying to contribute to a beginner-friendly issue when the maintainer told me to \u0026ldquo;make a PR\u0026rdquo; and I didn\u0026rsquo;t know that PR actually is the short form for Pull Request.\nTo help you avoid the hurdles I faced and prevent time-consuming mistakes that I made while beginning my open-source contributions journey, I have curated a list of essential terms one must know, in order to begin his/her open-source contributions journey.\nBefore that one very important tip - Open-source is not about spoon-feeding, so you will have to learn things by yourself, by making mistakes and learning from them. Therefore is completely fine that you don\u0026rsquo;t understand what to do. how to raise an issue? how to merge a PR? Open-source is a community of the most supportive people in the world and everyone would love to help you once you get stuck at some place. So try out, fail and iterate, don\u0026rsquo;t expect people won\u0026rsquo;t answer you or think you are dumb. Don\u0026rsquo;t hesitate asking, but neither should you expect spoon-feeding.\nStarting from the very basics Git Git is a distributed version control system that helps developers manage and track changes in their code projects. It allows you to create a history of changes, switch between versions, and collaborate with others. Git stores code in repositories and track each change made, enabling easy branching, merging, and resolving conflicts. You can work offline and sync changes later. Git\u0026rsquo;s decentralized nature ensures multiple developers can work simultaneously without conflicts. It provides a reliable and efficient way to organize, share, and safeguard code, making it an essential tool for collaborative software development.\nGitHub GitHub is a web-based platform that hosts and manages Git repositories. GitHub will be the place where you will contribute to various open-source projects. It provides a centralized space for developers to collaborate on code projects. GitHub allows you to store your code, track changes, and work with others in a streamlined manner. It offers features like issue tracking, pull requests, and code review, facilitating collaboration and efficient development workflows. With GitHub, you can easily share your code, contribute to open-source projects, and showcase your work. It serves as a social platform for developers, fostering community engagement and making it a hub for version control and collaborative coding.\n","permalink":"http://localhost:1313/blog/open-source101/","summary":"\u003cp\u003eContributing to open-source projects as a beginner can be a little overwhelming. We come across many terms which might sound alien to us. I remember myself 3 months ago, trying to contribute to a beginner-friendly issue when the maintainer told me to \u0026ldquo;make a PR\u0026rdquo; and I didn\u0026rsquo;t know that PR actually is the short form for Pull Request.\u003c/p\u003e\n\u003cp\u003eTo help you avoid the hurdles I faced and prevent time-consuming mistakes that I made while beginning my open-source contributions journey, I have curated a list of essential terms one must know, in order to begin his/her open-source contributions journey.\u003c/p\u003e","title":"Open-Source 101: The basic must-know terms for an open-source beginner"},{"content":"OSI (Open Systems Interconnection) OSI (Open Systems Interconnection) model describes seven layers through which computer systems communicate over a network.\nRemember that, the modern Internet is not based on OSI, but on TCP/IP model (Transmission Control Protocol/ Internet Protocol)\nPhysical Layer: The physical layer is concerned with the physical transmission of data over a network. It defines the electrical, mechanical, and procedural aspects of the network hardware and cabling.\nData Link Layer: The data link layer establishes and maintains reliable links between adjacent nodes on a network. It handles the physical addressing of data packets and error detection and correction.\nNetwork Layer: The network layer is responsible for logical addressing and routing of data between different networks. It determines the best path for data packets to travel from source to destination.\nTransport Layer: The transport layer provides end-to-end communication services for applications. It ensures that data is transmitted reliably, efficiently, and securely between communicating devices.\nSession Layer: The session layer manages and coordinates communication sessions between applications on different devices. It establishes, maintains, and terminates communication sessions.\nPresentation Layer: The presentation layer defines the format and representation of data exchanged between applications. It manages data compression, encryption, and decryption.\nApplication Layer: The application layer provides services directly to end-users. It specifies how user applications interact with the network and enables users to access network resources and services.\nOverall, the OSI model helps to streamline network communication by breaking the process down into smaller and manageable components. By having a clear understanding of the OSI model and how it works, network engineers can work more effectively and efficiently to design, implement, manage, and troubleshoot networks.\n","permalink":"http://localhost:1313/blog/osi/","summary":"\u003ch2 id=\"osi-open-systems-interconnection\"\u003eOSI (Open Systems Interconnection)\u003c/h2\u003e\n\u003cp\u003eOSI (Open Systems Interconnection) model describes seven layers through which computer systems communicate over a network.\u003c/p\u003e\n\u003cp\u003eRemember that, the modern Internet is not based on OSI, but on TCP/IP model (Transmission Control Protocol/ Internet Protocol)\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003ePhysical Layer:\u003c/strong\u003e\u003c/em\u003e The physical layer is concerned with the physical transmission of data over a network. It defines the electrical, mechanical, and procedural aspects of the network hardware and cabling.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eData Link Layer:\u003c/strong\u003e\u003c/em\u003e The data link layer establishes and maintains reliable links between adjacent nodes on a network. It handles the physical addressing of data packets and error detection and correction.\u003c/p\u003e","title":"OSI Model - the easy way"},{"content":"DevOps can be a bit confusing for beginners, that is because of its complex terminologies. Here are some terms you must know as a DevOps beginner.\nAgile: is an iterative approach to project management and software development that helps teams deliver value to their customers faster and with fewer issues.\nClient-server architecture: is a distributed application structure that partitions tasks or workloads between the providers of a resource or service, called servers, and service requesters, called clients.\nA container: powered by the containerization engine, is a standard unit of software that encapsulates the application code, runtime, system tools, system libraries, and settings necessary for programmers to efficiently build, ship and run applications.\nContainer Registry: Used for the storage and distribution of named container images. While many features can be built on top of a registry, its most basic functions are to store images and retrieve them.\nContainer Registry: Used for the storage and distribution of named container images. While many features can be built on top of a registry, its most basic functions are to store images and retrieve them.\nCI/CD pipelines: A continuous integration and continuous deployment (CI/CD) pipeline is a series of steps that must be performed in order to deliver a new version of software. CI/CD pipelines are a practice focused on improving software delivery throughout the software development life cycle via automation.\nCloud native: A cloud-native application is a program that is designed for a cloud computing architecture. These applications are run and hosted in the cloud and are designed to capitalize on the inherent characteristics of a cloud computing software delivery model.\nDaemon-less: A container runtime that does not run any specific program (daemon) to create objects, such as images, containers, networks, and volumes.\nDevOps: is a set of practices, tools, and a cultural philosophy that automate and integrate the processes between software development and IT teams.\nDocker: An open container platform for developing, shipping and running applications in containers.\nA Dockerfile: is a text document that contains all the commands you would normally execute manually in order to build a Docker image. Docker can build images automatically by reading the instructions from a Dockerfile.\nDocker client: is the primary way that many Docker users interact with Docker. When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon.\nDocker Command Line Interface (CLI) The Docker client provides a command line interface (CLI) that allows you to issue build, run, and stop application commands to a Docker daemon.\nDocker daemon (dockerd) creates and manages Docker objects, such as images, containers, networks, and volumes.\nDocker Hub: is the world\u0026rsquo;s easiest way to create, manage, and deliver your team\u0026rsquo;s container applications.\nDocker localhost: Docker provides a host network which lets containers share your host’s networking stack. This approach means that a localhost in a container resolves to the physical host, instead of the container itself.\nDocker remote host: A remote Docker host is a machine, inside or outside our local network which is running a Docker Engine and has ports exposed for querying the Engine API.\nDocker networks: help isolate container communications.\nDocker plugins: such as a storage plugin, provides the ability to connect external storage platforms.\nDocker storage: uses volumes and bind mounts to persist data even after a running container is stopped.\nLXC: LinuX Containers is a OS-level virtualization technology that allows creation and running of multiple isolated Linux virtual environments (VE) on a single control host.\nIBM Cloud Container Registry: stores and distributes container images in a fully managed private registry.\nImage: An immutable file that contains the source code, libraries, and dependencies that are necessary for an application to run. Images are templates or blueprints for a container.\nImmutability: Images are read-only; if you change an image, you create a new image.\nMicroservices: are a cloud-native architectural approach in which a single application contains many loosely coupled and independently deployable smaller components or services.\nNamespace: A Linux namespace is a Linux kernel feature that isolates and virtualizes system resources. Processes which are restricted to a namespace can only interact with resources or processes that are part of the same namespace. Namespaces are an important part of Docker’s isolation model. Namespaces exist for each type of resource, including networking, storage, processes, hostname control and others.\nOperating System Virtualization: OS-level virtualization is an operating system paradigm in which the kernel allows the existence of multiple isolated user space instances, called containers, zones, virtual private servers, partitions, virtual environments, virtual kernels, or jails.\nPrivate Registry: Restricts access to images so that only authorized users can view and use them.\nREST API: A REST API (also known as RESTful API) is an application programming interface (API or web API) that conforms to the constraints of REST architectural style and allows for interaction with RESTful web services.\nRegistry: is a hosted service containing repositories of images which responds to the Registry API.\nRepository: is a set of Docker images. A repository can be shared by pushing it to a registry server. The different images in the repository can be labelled using tags.\nServer Virtualization: Server virtualization is the process of dividing a physical server into multiple unique and isolated virtual servers by means of a software application. Each virtual server can run its own operating systems independently.\nServerless: is a cloud-native development model that allows developers to build and run applications without having to manage servers.\nTag: A tag is a label applied to a Docker image in a repository. Tags are how various images in a repository are distinguished from each other.\n","permalink":"http://localhost:1313/blog/devops/","summary":"\u003cp\u003eDevOps can be a bit confusing for beginners, that is because of its complex terminologies. Here are some terms you must know as a DevOps beginner.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eAgile:\u003c/strong\u003e\u003c/em\u003e is an iterative approach to project management and software development that helps teams deliver value to their customers faster and with fewer issues.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eClient-server architecture:\u003c/strong\u003e\u003c/em\u003e is a distributed application structure that partitions tasks or workloads between the providers of a resource or service, called servers, and service requesters, called clients.\u003c/p\u003e","title":"Some DevOps terms a beginner should know about"},{"content":"We often come through JSON files during developing web applications. you must have noticed that the JSON files store data in the form of a bunch of key-value pairs. JSON is often used in web development frameworks such as React and Angular, as well as in backend frameworks such as Node.js and Django.\nJSON JSON stands for JavaScript Object Notation. It is a lightweight data-interchange format that is easy for humans to read and write, and easy for machines to parse and generate.\nWhy is it so widely used? JSON is very lightweight, easy to read and integrates smoothly with JavaScript. That\u0026rsquo;s the main reason why it is used mostly in web dev, rather than XML. JSON is often used to transmit data between a server and a web application, as an alternative to XML. In JSON, data is represented as key-value pairs, similar to objects in many programming languages.\n{\r\u0026#34;name\u0026#34;: \u0026#34;Shubham\u0026#34;,\r\u0026#34;age\u0026#34;: 19,\r\u0026#34;isMarried\u0026#34;: false,\r\u0026#34;skills\u0026#34;: [\u0026#34;web dev\u0026#34;, \u0026#34;devops\u0026#34;, \u0026#34;android dev\u0026#34;]\r} JSON was first introduced in 2002 by Douglas Crockford, who is also known for his work on JavaScript and the development of the JSON standard. Some alternatives to JSON are XML (Extensible Markup Language), and YAML (Yet Another Markup Language).\nThis marks the end of this article, Thanks for your time. Do check out my other articles on tech-related terminologies.\n","permalink":"http://localhost:1313/blog/json/","summary":"\u003cp\u003eWe often come through JSON files during developing web applications. you must have noticed that the JSON files store data in the form of a bunch of key-value pairs. JSON is often used in web development frameworks such as React and Angular, as well as in backend frameworks such as Node.js and Django.\u003c/p\u003e\n\u003ch2 id=\"json\"\u003eJSON\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eJSON stands for JavaScript Object Notation. It is a lightweight data-interchange format that is easy for humans to read and write, and easy for machines to parse and generate.\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e","title":"WTF is JSON"},{"content":"You might remember using npm start in your React app or using pip in python. Both npm and pip are package managers of their respective languages. Therefore the technical definition of package manager is -\n\u0026ldquo;A package manager is a tool used in software development to manage the installation, updating, and removal of software packages or libraries.\u0026rdquo;\nPackage managers automate the process of searching, installing, and maintaining these packages, which makes it easier for developers to include them in their projects. Instead of manually downloading and installing each package separately, developers can use the package manager to handle all the dependencies and ensure that the necessary versions are installed.\nThere are two main types of package managers:\nSystem-Level Package Managers: These package managers are used to manage packages at the operating system level. Examples of system-level package managers include Aptitude (used in Debian and Ubuntu), YUM (used in CentOS and Fedora), and Homebrew (used in macOS).\nLanguage-Specific Package Managers: These package managers are used to manage packages specific to a programming language. Examples of language-specific package managers include npm (used in Node.js), pip (used in Python), Composer (used in PHP), and NuGet (used in .NET).\nIn conclusion, a package manager is a tool that automates the process of managing software dependencies by allowing developers to search, install, and update packages or libraries easily. The use of a package manager saves time and reduces duplication of effort by automating the handling of dependencies. Almost every programming language has its own package manager, and using one has become a critical part of modern software development workflows.\n","permalink":"http://localhost:1313/blog/npm/","summary":"\u003cp\u003eYou might remember using npm start in your React app or using pip in python. Both npm and pip are package managers of their respective languages. Therefore the technical definition of package manager is -\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003e\u0026ldquo;A package manager is a tool used in software development to manage the installation, updating, and removal of software packages or libraries.\u0026rdquo;\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003ePackage managers automate the process of searching, installing, and maintaining these packages, which makes it easier for developers to include them in their projects. Instead of manually downloading and installing each package separately, developers can use the package manager to handle all the dependencies and ensure that the necessary versions are installed.\u003c/p\u003e","title":"WTF is JSON"},{"content":"So recently, I was setting up locally an open-source software, so to contribute to it (BTW it is called Zulip, Please check it out - it\u0026rsquo;s an amazing org). And its documentation said me to download Vagrant in order to run it locally on Windows. I researched about it and this is what I got to know -\n\u0026ldquo;Vagrant is a tool that helps developers to create and manage virtual machines (VMs) or lightweight, reproducible development environments.\u0026rdquo;\nIt is an open-source tool. It automates the process of creating and configuring virtual machines, making it easier for developers to work on multiple projects with different system requirements on a single machine.\nWith Vagrant, you can use provisioners to install software, configure environments, and integrate with other tools like Ansible, Chef, and Puppet. You can also share and distribute your VMs easily with your team or community, making it a popular choice for collaborative workflows.\nVagrant can be used with various virtualization technologies, including VirtualBox, VMware, and Hyper-V, on Windows, macOS, and Linux platforms.\nAnd that\u0026rsquo;s it. Thank you for your time. Follow my blog for more such articles.\n","permalink":"http://localhost:1313/blog/vagrant/","summary":"\u003cp\u003eSo recently, I was setting up locally an open-source software, so to contribute to it (BTW it is called Zulip, Please check it out - it\u0026rsquo;s an amazing org). And its documentation said me to download Vagrant in order to run it locally on Windows. I researched about it and this is what I got to know -\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003e\u0026ldquo;Vagrant is a tool that helps developers to create and manage virtual machines (VMs) or lightweight, reproducible development environments.\u0026rdquo;\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e","title":"WTF is Vagrant"},{"content":"What is YAML? YAML (which stands for \u0026ldquo;YAML Ain\u0026rsquo;t Markup Language\u0026rdquo;) is a human-readable data serialization language. We will talk later about what a serialization language is.\nYAML is used for configuration files, exchanging data between programming languages and storing data. YAML files are extensively used in the DevOps field. YAML is considered better than its alternative because of its easy-to-read way.\nIt is often used for configuration files, data exchange between programming languages, and storing data in a structured and easy-to-read way.\nYAML files are denoted by \u0026quot;.YML\u0026quot; OR \u0026quot;.YAML\u0026quot;.\nInitially, YAML was called \u0026ldquo;Yet Another Markup Language\u0026rdquo;. but the name was changed to simply \u0026ldquo;YAML Ain\u0026rsquo;t Markup Language\u0026rdquo; (or just YAML) in order to emphasize that YAML is not just another markup language like XML or HTML, but rather a data serialization language that is more human-readable and easier to work with.\nExample of YAML file: This file includes almost all important YAML concepts. It\u0026rsquo;s a bit lengthy but very important to understand the concepts of YAML. Do check it out.\n\u0026#34;name\u0026#34;: \u0026#34;Shubham\u0026#34;\r# key value pairs\r---\r# lists\r\u0026#34;list\u0026#34;:\r- \u0026#34;apple\u0026#34;\r- \u0026#34;mango\u0026#34;\r- \u0026#34;three\u0026#34;\r- four\r---\r# documents are seperated by ---\r# \u0026#34;...\u0026#34; denotes that documents has ended\r# String variables -\u0026gt;\ra: \u0026#34;string\u0026#34;\rb: string\rc: \u0026#39;string\u0026#39;\rfloat: 10.12\rint: 23\rboolean: No #n , N, false, False, FALSE\rbooleanTwo: Yes #y, Y, True, true, TRUE\rnulltext: Null #null NULL\r---\rd: |\rThis is how we write multi line data\re: \u0026gt;\rThis is how we write\rone line data in multiple lines\r# specify the type:\rzero: !!int 0\rcommaValue: !!int +100_000 #equivalent to 100,000\rbooleanThree: !!bool Yes\rstringtwo: !!str \u0026#34;Shubham\u0026#34;\rfloatTwo: !!float 12.12\rnulltext: !!null Null #null NULL\r# maps: key value pairs\rmap example:\rname: Shubham\rage: 19\rcountry: India\r# another way of representing a map\rmap example two: {name: Shubham, age: 19, country: India}\r# sets are used to store items\rexpertise: !!set\r? Web dev\r? Android dev\r? DevOps\r# Dictionary or !!omap\rstudents: !!omap\r- Shubham:\rage: 19\r- Ram:\rage: 23\r- Shayam:\rage: 24\r# Using anchors\rage_and_gender: \u0026amp;19_and_male\rgender: male\rage: 19\rperson1:\rname: Shubham\r\u0026lt;\u0026lt;: *19_and_male\rperson2:\rname: Ram\r\u0026lt;\u0026lt;: *19_and_male Data serialization: Data serialization is the process of converting data from one format to another. When data is serialized, it is transformed into a format that can be easily stored or transmitted across different applications or systems.\nThe reverse process of converting the serialized data back to its original format is called deserialization.\nFor Data serialization, we use the data serialization formats like JSON, YAML and XML. YAML is one of the more popular options out of these. JSON and YAML are more human-readable than XML.\nAdvantages of YAML: Here are some benefits of YAML:\nHuman-readable: One of the biggest benefits of YAML is that it is easy for humans to read and write. Unlike JSON or XML, YAML documents use simple indentation and whitespace to structure data, making them easier to understand and modify.\nSupports complex data structures: YAML supports a wide range of data structures, including lists, dictionaries, and nested structures, making it ideal for working with complex data.\nLanguage agnostic: YAML is language agnostic, meaning that it can be used with any programming language without requiring any special libraries or tools.\nLess verbose: YAML generally requires less syntax compared to other data serialization formats like JSON or XML, making it cleaner and more concise.\nSupports comments: YAML supports comments, which allow developers to add notes and explanations within the document.\nGreat for configuration files: YAML is commonly used for configuration files, such as those used in Docker or Kubernetes, because of its simplicity and readability.\nThese benefits make YAML a popular choice for developers looking for a data serialization format that is easy to work with and supports a wide range of data structures.\nConclusion: In conclusion, YAML is a simple, human-readable data serialization format that allows developers to easily exchange and store data across different programming languages and systems. Its support for complex data structures, language agnosticism, and support for comments, make it an ideal choice for developers working in the DevOps field or those dealing with configuration files. With its intuitive syntax and streamlining of complex data structures, YAML is becoming a popular choice for developers looking to improve readability and maintainability of their code.\n","permalink":"http://localhost:1313/blog/yaml/","summary":"\u003ch2 id=\"what-is-yaml\"\u003eWhat is YAML?\u003c/h2\u003e\n\u003cp\u003eYAML (which stands for \u0026ldquo;YAML Ain\u0026rsquo;t Markup Language\u0026rdquo;) is a human-readable data serialization language. We will talk later about what a serialization language is.\u003c/p\u003e\n\u003cp\u003eYAML is used for configuration files, exchanging data between programming languages and storing data. YAML files are extensively used in the DevOps field. YAML is considered better than its alternative because of its easy-to-read way.\u003c/p\u003e\n\u003cp\u003eIt is often used for configuration files, data exchange between programming languages, and storing data in a structured and easy-to-read way.\u003c/p\u003e","title":"YAML - from basics to advanced"},{"content":"During our Web development journey will all get to hear the term - \u0026ldquo;API\u0026rdquo;. Some of us must have used some APIs, but not many know about what API actually is and how to play around with API. Join me in this article where we dive deep into the world of API and understand the concepts of API from beginner to advanced.\nWhat is API? The full form of API is Application Programming Interface. In simplest terms, API is a contract that allows code to talk to some other code. It serves as a bridge that enables a seamless exchange of data and functionality between various systems, enabling developers to access and utilize the features of another application or service without having to understand its internal workings.\nAPIs define a standardized way for different software components to interact, providing a layer of abstraction that simplifies development and integration. They typically consist of a collection of pre-defined functions, classes, or endpoints that developers can use to request or exchange data, perform operations, or access specific functionalities of the underlying system.\nWhy do we need API? It\u0026rsquo;s very basic, for making a weather app - would you launch satellites or just use a weather API?\nSome more examples of why we need API are:\nSocial Media Integration: Social media platforms like Facebook, Twitter, and Instagram provide APIs that allow developers to integrate their applications with these platforms. This integration enables users to log in using their social media accounts, share content, and access social features within the application.\nPayment Gateways: APIs offered by payment gateway providers such as PayPal, Stripe, and Braintree allow developers to incorporate secure payment processing functionality into their applications. This enables users to make online payments using different payment methods.\nMapping and Geolocation Services: APIs provided by mapping and geolocation services like Google Maps and Mapbox enable developers to embed maps, geocoding, and routing functionality into their applications. These APIs provide features such as displaying locations, calculating distances, and finding directions.\nWeather Data: Weather APIs, such as those offered by OpenWeatherMap and Weather Underground, provide access to real-time and forecast weather data. Developers can utilize these APIs to display weather information within their applications, enabling users to check current weather conditions or plan for future events.\nE-commerce Platforms: APIs offered by e-commerce platforms like Shopify and WooCommerce allow developers to create online stores, manage inventory, process orders, and retrieve product information. These APIs enable seamless integration between e-commerce platforms and custom applications.\nEmail Services: Email service providers like SendGrid and Mailchimp offer APIs that allow developers to send and manage emails programmatically. This enables applications to send transactional emails, newsletters, and marketing campaigns.\nCloud Services: Cloud computing providers such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform offer APIs for various services like storage, computing, and database management. Developers can leverage these APIs to build scalable and flexible cloud-based applications.\nAPI Architectures There are several types of API architectures commonly used in web development. Here are three prominent ones:\nREST (Representational State Transfer) REST is a widely adopted architectural style for designing networked applications. RESTful APIs utilize the principles of the HTTP protocol and leverage its methods (GET, POST, PUT, DELETE, etc.) to perform operations on resources. REST APIs typically use URLs (Uniform Resource Locators) to identify resources and employ different HTTP status codes to indicate the outcome of a request. They emphasize statelessness, scalability, and interoperability, making them popular for building web services.\nSOAP (Simple Object Access Protocol) SOAP is an XML-based protocol that enables communication between applications over a network. SOAP APIs define a strict structure for request and response messages using XML schemas. They rely on the XML format for data representation and typically use the POST method for communication. SOAP APIs often employ Web Services Description Language (WSDL) to describe the available operations, message formats, and service endpoints. SOAP APIs are known for their strong message-level security and support for advanced features such as transactions and reliability.\nGraphQL GraphQL is an open-source query language and runtime for APIs developed by Facebook. It allows clients to request precisely the data they need, eliminating over-fetching or under-fetching of data common in traditional REST APIs. With GraphQL, clients can send queries specifying the desired data structure, and the server responds with a JSON payload containing only the requested data. This flexible and efficient approach to data fetching makes GraphQL popular for applications with complex data requirements and enables clients to aggregate data from multiple sources in a single request.\nThese are just a few examples of API architectures, and there are other variations and hybrid approaches as well. The choice of API architecture depends on factors such as the project requirements, scalability needs, interoperability considerations, and the preferences of the development team.\nConclusion In conclusion, APIs are the lifeblood of modern web development, enabling seamless communication and integration between different applications and services. They provide a standardized way for software components to interact, allowing developers to leverage the functionality of external systems without needing to understand their internal complexities.\nThroughout this article, we\u0026rsquo;ve explored the world of APIs from beginner to advanced levels. We\u0026rsquo;ve learned that APIs serve as the bridge that connects applications, enabling us to incorporate social media integration, payment gateways, mapping services, weather data, e-commerce functionalities, email services, and cloud computing capabilities into our own projects with ease.\nWe\u0026rsquo;ve also touched upon different API architectures, such as REST, SOAP, and GraphQL, each with its own strengths and areas of application. Understanding these architectures empowers us to make informed decisions when designing and implementing APIs in our projects.\nHope you gained some valuable insights. Do follow me for more such articles.\nThank you for your time.\n","permalink":"http://localhost:1313/blog/api/","summary":"\u003cp\u003eDuring our Web development journey will all get to hear the term - \u0026ldquo;API\u0026rdquo;. Some of us must have used some APIs, but not many know about what API actually is and how to play around with API. Join me in this article where we dive deep into the world of API and understand the concepts of API from beginner to advanced.\u003c/p\u003e\n\u003ch2 id=\"what-is-api\"\u003eWhat is API?\u003c/h2\u003e\n\u003cp\u003eThe full form of API is Application Programming Interface. In simplest terms, API is a contract that allows code to talk to some other code. It serves as a bridge that enables a seamless exchange of data and functionality between various systems, enabling developers to access and utilize the features of another application or service without having to understand its internal workings.\u003c/p\u003e","title":"Everything about API explained!!"},{"content":"🔗 Code Buddy Description Code Buddy is a place for coders to find their buddies - find a coding partner for hackathons, events and just chilling out. find programmers near you, Sort by location, technology, and shared interests.\nTech Stack used - React Golang ","permalink":"http://localhost:1313/projects/code-buddy/","summary":"\u003ch3 id=\"-code-buddyhttpsbode-buddynetlifyapp\"\u003e🔗 \u003ca href=\"https://bode-buddy.netlify.app/\"\u003eCode Buddy\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eCode Buddy is a place for coders to find their buddies - find a coding partner for hackathons, events and just chilling out. find programmers near you, Sort by location, technology, and shared interests.\u003c/p\u003e\n\u003ch2 id=\"tech-stack-used--\"\u003eTech Stack used -\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eReact\u003c/li\u003e\n\u003cli\u003eGolang\u003c/li\u003e\n\u003c/ul\u003e","title":"Code Buddy"},{"content":"web server https://github.com/1Shubham7/web-server ","permalink":"http://localhost:1313/projects/web-server/","summary":"\u003ch2 id=\"web-server\"\u003eweb server\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/1Shubham7/web-server\"\u003ehttps://github.com/1Shubham7/web-server\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"Web Server"},{"content":"","permalink":"http://localhost:1313/projects/mission-health/","summary":"","title":""}]